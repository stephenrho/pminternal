---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "90%"
)
```

# pminternal: Internal Validation of Clinical Prediction Models

<!-- badges: start -->
<!-- badges: end -->

This is a work in progress. 

The goal is to offer a package that can produce bias-corrected performance measures for binary outcomes for a range of model development approaches available in R (similar to `rms::validate`). Also contains function for assessing prediction stability as described here https://doi.org/10.1002/bimj.202200302.  

To install development version:

``` r
# install.packages("devtools")
devtools::install_github("stephenrho/pminternal", build_vignettes = TRUE)
```

Please send feedback to steverho89@gmail.com or open an issue. 

## Example

In the example below we use bootstrapping to correct performance measures for a `glm` via calculation of 'optimism' (see `vignette("pminternal")` and `vignette("validate-examples")` for more examples):

```{r example}
library(pminternal)

# make some data
set.seed(2345)
n <- 800
p <- 10

X <- matrix(rnorm(n*p), nrow = n, ncol = p)
LP <- -1 + apply(X[, 1:5], 1, sum) # first 5 variables predict outcome
y <- rbinom(n, 1, plogis(LP))

dat <- data.frame(y, X)

# fit a model
mod <- glm(y ~ ., data = dat, family = "binomial")

# calculate bootstrap optimism corrected performance measures
(val <- validate(fit = mod, method = "boot_optimism", B = 100))

# other models can be supplied to fit
# or users can specify 'model_fun' 
# see vignette("pminternal") and vignette("validate-examples")

```

The other available methods for calculating bias corrected performance are the simple bootstrap (`boot_simple`), 0.632 bootstrap optimism (`.632`), optimism via cross-validation (`cv_optimism`), and regular cross-validation (`cv_average`). Please see `?pminternal::validate` and the references therein. Bias corrected calibration curves can also be produced (see `cal_plot`).

The output of `validate` (with `method = "boot_*"`) can be used to produce plots for assessing the stability of model predictions (across models developed on bootstrap resamples).

A prediction (in)stability plot shows predictions from the `B` (in this case 100) bootstrap models applied to the development data.

```{r stability}
prediction_stability(val)
```

A MAPE plot shows the mean absolute prediction error, which is the difference between the predicted risk from the development model and each of the `B` bootstrap models.

```{r mape}
mape_stability(val)
```

A calibration (in)stability plot depict the original calibration curve along with `B` calibration curves from the bootstrap models applied to the original data (`y`).

```{r cal}
calibration_stability(val)
```

The classification instability index (CII) is the proportion of individuals that change predicted class (present/absent, 1/0) when predicted risk is compared to some threshold. For example, a patient predicted to be in class 1 would receive a CII of 0.3 if 30% of the bootstrap models led to a predicted class of 0. 

```{r CII}
classification_stability(val, threshold = .4)
```

Decision curves implied by the original and bootstrap models can also be plotted.

```{r dcurve}
dcurve_stability(val)
```
