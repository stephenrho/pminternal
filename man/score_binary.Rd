% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score_binary.R
\name{score_binary}
\alias{score_binary}
\title{Score predictions for binary events}
\usage{
score_binary(y, p, ...)
}
\arguments{
\item{y}{vector containing a binary outcome}

\item{p}{vector of predictions}

\item{...}{additional arguments. This function only supports calib_args as
an optional argument. calib_args should contain arguments for pmcalibration::pmcalibration.
If a calibration plot (apparent vs bias corrected calibration curves via \code{\link{cal_plot}})
is desired the argument 'eval' should be provided. This should be the points at which to evaluate
the calibration curve on each boot resample or crossvalidation fold. A good option would be
calib_args = list(eval = seq(min(p), max(p), length.out=100)); where p are predictions from the
original model evaluated on the original data. Dots can be used to supply additional arguments to
user-defined functions.}
}
\value{
a named vector of scores
}
\description{
Calculate scores summarizing discrimination/calibration of predictions
against observed binary events. If score_fun is not defined when calling
\code{\link{validate}} this function is used.
}
\examples{
p <- runif(100)
y <- rbinom(length(p), 1, p)
score_binary(y = y, p = p)
}
