[{"path":"https://stephenrho.github.io/pminternal/articles/missing-data.html","id":"stacked-multiple-imputation","dir":"Articles","previous_headings":"","what":"Stacked multiple imputation","title":"Handling missing data","text":"approach discussed Janssen et al. (2009) Hoogland et al. (2020) involves data used ‘training’ model available validation. Multiple imputation used development model coefficients pooled across mm imputed data sets. validation testing, new data stacked onto data used development multiple imputation algorithm run combined data set. two articles point important new data validation dominate imputation model therefore advocate stacked imputation one row validation set time. use ignore argument mice::mice omit validation data imputation model still impute missing observations. individual validation/test dataset linear predictors obtained imputed dataset averaged using inverse logit transform convert risk. Note functions use mice default arguments (5 imputed datasets via predictive mean matching predictors continuous) easily tweaked. functions also assume variables used imputation model (including y) changed via predictorMatrix. drawback approach requires development data available validation/application. avoid possible issues sharing development data, might possible simulate data properties missingness patterns supply prediction model deployment (assuming MI algorithms also made available), although (knowledge) approach hasn’t assessed.","code":"model_mi <- function(data, ...){   imp <- mice::mice(data, printFlag = FALSE) # 5 imputed datasets via pmm   fits <- with(imp, glm(y ~ X1 + X2 + X3 + X4 + X5, family=binomial))   B <- mice::pool(fits)$pooled$estimate # pooled coefs for predictions   # save pooled coefficients and the data to do stacked imputation at validation   list(B, data)  }  pred_mi <- function(model, data, ...){   # extract pooled coefs   B <- model[[1]]   # stack the new data on the model fit data (model[[2]])   dstacked <- rbind(model[[2]], data)   i <- (nrow(model[[2]]) + 1):(nrow(dstacked))   # impute stacked data   # use ignore argument to ensure the test data doesn't influence   # the imputation model but still gets imputed   imp <- mice::mice(dstacked, printFlag = FALSE,                      ignore = seq(nrow(dstacked)) %in% i)   # get logit predicted risks for each imputed data set   preds <- sapply(seq(imp$m), \\(x){     # complete(imp, x)[i, ] = get complete data set x and extract the      # validation data i     X <- model.matrix(~ X1 + X2 + X3 + X4 + X5, data = mice::complete(imp, x)[i, ])      X %*% B   })   # average logit predictions, transform invlogit, and return   plogis(apply(preds, 1, mean))  } (mi_val <- validate(data = datmis,                      model_fun = model_mi, pred_fun = pred_mi,                      outcome = \"y\", method = \"cv_o\")) #>           apparent optimism corrected  n #> C            0.821   0.0020     0.819 10 #> Brier        0.160  -0.0007     0.161 10 #> Intercept    0.221   0.0058     0.216 10 #> Slope        1.267  -0.0446     1.312 10 #> Eavg         0.038  -0.0330     0.071 10 #> E50          0.034  -0.0202     0.054 10 #> E90          0.079  -0.0591     0.138 10 #> Emax         0.084  -0.1489     0.232 10 #> ECI          0.200  -0.9423     1.142 10"},{"path":"https://stephenrho.github.io/pminternal/articles/missing-data.html","id":"pattern-submodels","dir":"Articles","previous_headings":"","what":"Pattern submodels","title":"Handling missing data","text":"approach involves fitting separate prediction model pattern missing data using data pattern (Mercaldo & Blume, 2020). example, pattern “00100” signifies variable 3 missing others observed, model omitting variable 3 fit using rows missing pattern “00100”. testing model new data model corresponding missing data pattern new observation used. shown 8 missing data patterns.  code creates new column datamis containing observed missing data pattern, mdp. list submodels contains models estimated using data missing data pattern. number submodels large (2p2^{p}) created programmatically, though large number submodels computationally prohibitive. Following Mercaldo & Blume, model_ps pattern submodel estimated least 2p2p observations given pattern (case p=5p = 5), otherwise submodel estimated using observations complete data given submodel (‘complete case submodel’). important note n>2pn > 2p patterns development data guaranteed resampling bootstrap CV. possible include patterns observed development data, although little value internal validation use observed patterns.","code":"md <- md.pattern(datmis) # store missing data pattern in data # as this will be useful datmis$mdp <- apply(datmis[, paste0(\"X\", 1:5)], 1, \\(x) paste0(as.numeric(is.na(x)), collapse = \"\"))  table(datmis$mdp) #>  #> 00000 00100 01000 01100 10000 10100 11000 11100  #>   344   107   108    41   113    36    35    16 submodels <- list(   \"00000\" = y ~ X1 + X2 + X3 + X4 + X5,   \"00100\" = y ~ X1 + X2 + X4 + X5,   \"01000\" = y ~ X1 + X3 + X4 + X5,   \"01100\" = y ~ X1 + X4 + X5,   \"10000\" = y ~ X2 + X3 + X4 + X5,   \"10100\" = y ~ X2 + X4 + X5,   \"11000\" = y ~ X3 + X4 + X5,   \"11100\" = y ~ X4 + X5 )  model_ps <- function(data, ...){   # this example uses submodels as an additional argument to    # validate. But we could have put the submodels list in the    # model_sub function   dots <- list(...)   if (\"submodels\" %in% names(dots))      submodels <- dots[[\"submodels\"]]    else      stop(\"no submodels\")      patterns <- names(submodels)   # all patterns in data should be in submodels,    # but not necessarily vice versa (resampling    # could have omitted a pattern)   stopifnot(all(unique(data$mdp) %in% patterns))       fits <- lapply(patterns, \\(pat){     f <- submodels[[pat]] # submodel formula     # if n with pattern > 2*p then fit submodel     n <- sum(data$mdp == pat)     if (n >= 2*nchar(pat)){       fit <- glm(f, data = subset(data, mdp == pat), family = binomial)     } else{       # otherwise fit on complete cases       fit <- glm(f, data = data, family = binomial)       # note this approach allows submodels for patterns not        # observed in development data. Though these will be        # fit using complete cases (obviously not pattern specific data)     }     fit   })      names(fits) <- patterns   fits }  pred_ps <- function(model, data, ...){   patterns <- names(model)   # there needs to be a model for each pattern in data   stopifnot(all( unique(data$mdp) %in% patterns ))      patterns <- patterns[patterns %in% unique(data$mdp)]      preds <- lapply(patterns, \\(pat){     i = which(data$mdp == pat) # for reordering later     pdat <- subset(data, mdp == pat)     p <- predict(model[[pat]], newdata = pdat, type = \"response\")     data.frame(y = pdat$y, p = p, i = i)   })      preds <- do.call(rbind, preds)   # reorder so same as original data   preds <- preds[order(preds$i),]   # all(data$y == preds$y)      preds$p } (sub_val <- validate(data = datmis,                      model_fun = model_ps, pred_fun = pred_ps,                      submodels = submodels,                      outcome = \"y\", method = \"cv_o\")) #>           apparent optimism corrected  n #> C         0.769339   0.0029     0.766 10 #> Brier     0.176129   0.0000     0.176 10 #> Intercept 0.000000  -0.0201     0.020 10 #> Slope     1.000000  -0.0521     1.052 10 #> Eavg      0.000001  -0.0458     0.046 10 #> E50       0.000001  -0.0403     0.040 10 #> E90       0.000001  -0.0856     0.086 10 #> Emax      0.000002  -0.1358     0.136 10 #> ECI       0.000000  -0.5370     0.537 10"},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting started with `pminternal`","text":"developing clinical prediction model measures model performance biased fact ’re using data fit (‘train’) model evaluate . Splitting data development validation sets inefficient. Bootstrapping cross-validation can used estimate bias-corrected measures model performance. known ‘internal validation’ addresses question: expected performance model developed way sample selected population? confused ‘external validation’ assesses model performance different population setting. pminternal inspired functions validate predab.resample rms package. aim provide package work user-defined model development procedure (assuming can implemented R function). package also implements recently proposed ‘stability plots’. Currently binary outcomes supported goal eventually extend outcomes (survival, ordinal).","code":""},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"supplying-a-model-via-fit","dir":"Articles","previous_headings":"","what":"Supplying a model via fit","title":"Getting started with `pminternal`","text":"validate needs single argument run, fit. fit fitted model compatible insight::get_data, insight::find_response, insight::get_call, marginaleffects::get_predict. Models supported insight can found running insight::supported_models() (run is_model_supported(fit)); models supported marginaleffects https://marginaleffects.com/bonus/supported_models.html. ’re dealing binary outcomes, models listed applicable. code loads GUSTO-trial data, selects relevant variables, subsets reduce run time, fits model (glm), passes validate produce optimism corrected performance metrics via bootstrap resampling. validate call run method = \"boot_optimism\" able assess model stability via following calls. Note stability plots based estimates optimism rather based predictions models developed bootstrapped resampled data sets evaluated original/development data. sense conceptually related bias-corrected estimates obtained method = \"boot_simple\". case methods results necessary data make plots (see also classification_stability dcurve_stability).    possible get apparent bias-corrected calibration curves. need set additional argument, specifying assess calibration curve (.e., points x-axis) follows. can also select calibration curves estimated. case use restricted cubic spline 5 knots (see pminternal::cal_defaults() default settings).  plotting functions fairly basic invisibly return data needed reproduce like. example, plot uses ggplot2 adds histogram predicted risk probabilities (stored p) show distribution.  Finally, bootstrap confidence intervals can calculated (see https://onlinelibrary.wiley.com/doi/10.1002/sim.9148 pminternal::confint.internal_validate details). confidence intervals available plotted cal_plot default.  Additional models supplied via fit tested gusto example given . Please let know run trouble model class feel work fit. chunk evaluated build time print output.","code":"library(pminternal) library(Hmisc) #>  #> Attaching package: 'Hmisc' #> The following objects are masked from 'package:base': #>  #>     format.pval, units  getHdata(\"gusto\") gusto <- gusto[, c(\"sex\", \"age\", \"hyp\", \"htn\", \"hrt\", \"pmi\", \"ste\", \"day30\")]  gusto$y <- gusto$day30; gusto$day30 <- NULL mean(gusto$y) # outcome rate #> [1] 0.06982611  set.seed(234) gusto <- gusto[sample(1:nrow(gusto), size = 4000),]  mod <- glm(y ~ ., data = gusto, family = \"binomial\")  mod_iv <- validate(mod, B = 20) #> It is recommended that B >= 200 for bootstrap validation mod_iv #>           apparent optimism corrected  n #> C           0.7983  0.00385   0.79446 20 #> Brier       0.0603 -0.00063   0.06094 20 #> Intercept   0.0000  0.00687  -0.00687 20 #> Slope       1.0000  0.00837   0.99163 20 #> Eavg        0.0023  0.00123   0.00108 20 #> E50         0.0014  0.00107   0.00034 20 #> E90         0.0044  0.00280   0.00160 20 #> Emax        0.0447  0.04581  -0.00112 20 #> ECI         0.0017  0.00704  -0.00530 20 # prediction stability plot with 95% 'stability interval' prediction_stability(mod_iv, bounds = .95) # calibration stability  # (using default calibration curve arguments: see pminternal:::cal_defaults()) calibration_stability(mod_iv) # mean absolute prediction error (mape) stability  # mape = average difference between boot model predictions # for original data and original model mape <- mape_stability(mod_iv) mape$average_mape #> [1] 0.009233709 # find 100 equally spaced points  # between the lowest and highest risk prediction p <- predict(mod, type=\"response\")  p_range <- seq(min(p), max(p), length.out=100)  mod_iv2 <- validate(mod, B = 20,                      calib_args = list(                       eval=p_range,                        smooth=\"rcs\",                        nk=5)                     ) #> It is recommended that B >= 200 for bootstrap validation mod_iv2 #>           apparent optimism corrected  n #> C           0.7983  0.00204    0.7963 20 #> Brier       0.0603 -0.00096    0.0613 20 #> Intercept   0.0000  0.02506   -0.0251 20 #> Slope       1.0000  0.01743    0.9826 20 #> Eavg        0.0068 -0.00061    0.0074 20 #> E50         0.0063  0.00021    0.0060 20 #> E90         0.0106 -0.00030    0.0109 20 #> Emax        0.0820 -0.00523    0.0873 20 #> ECI         0.0092 -0.00136    0.0105 20  calp <- cal_plot(mod_iv2) head(calp) #>     predicted    apparent bias_corrected #> 1 0.001661484 0.006418392    0.007972031 #> 2 0.009484478 0.007824631    0.008528307 #> 3 0.017307471 0.010911750    0.010198648 #> 4 0.025130465 0.017945390    0.017133285 #> 5 0.032953458 0.028541737    0.028377736 #> 6 0.040776452 0.040675895    0.041411019  library(ggplot2)  ggplot(calp, aes(x=predicted)) +   geom_abline(lty=2) +   geom_line(aes(y=apparent, color=\"Apparent\")) +   geom_line(aes(y=bias_corrected, color=\"Bias-Corrected\")) +   geom_histogram(data = data.frame(p = p), aes(x=p, y=after_stat(density)*.01),                  binwidth = .001, inherit.aes = F, alpha=1/2) +   labs(x=\"Predicted Risk\", y=\"Estimated Risk\", color=NULL) (mod_iv2 <- confint(mod_iv2, method = \"shifted\", R=100)) #>           apparent optimism corrected  n corrected_lower corrected_upper #> C           0.7983  0.00204    0.7963 20          0.7724           0.826 #> Brier       0.0603 -0.00096    0.0613 20          0.0548           0.066 #> Intercept   0.0000  0.02506   -0.0251 20         -0.0251          -0.025 #> Slope       1.0000  0.01743    0.9826 20          0.9826           0.983 #> Eavg        0.0068 -0.00061    0.0074 20          0.0039           0.013 #> E50         0.0063  0.00021    0.0060 20          0.0026           0.011 #> E90         0.0106 -0.00030    0.0109 20          0.0060           0.026 #> Emax        0.0820 -0.00523    0.0873 20          0.0161           0.198 #> ECI         0.0092 -0.00136    0.0105 20          0.0030           0.042 cal_plot(mod_iv2, bc_col = \"red\") ### generalized boosted model with gbm library(gbm) # syntax y ~ . does not work with gbm mod <- gbm(y ~ sex + age + hyp + htn + hrt + pmi + ste,             data = gusto, distribution = \"bernoulli\", interaction.depth = 2)  (gbm_iv <- validate(mod, B = 20))  ### generalized additive model with mgcv library(mgcv)  mod <- gam(y ~ sex + s(age) + hyp + htn + hrt + pmi + ste,             data = gusto, family = \"binomial\")  (gam_iv <- validate(mod, B = 20))  ### rms implementation of logistic regression mod <- rms::lrm(y ~ ., data = gusto)  # not loading rms to avoid conflict with rms::validate...  (lrm_iv <- validate(mod, B = 20))"},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"user-defined-model-development-functions","dir":"Articles","previous_headings":"","what":"User-defined model development functions","title":"Getting started with `pminternal`","text":"important internally validated entire model development procedure, including tuning hyperparameters, variable selection, . Often fit object capture (supported). example work model supported insight marginaleffects: logistic regression lasso (L1) regularization. functions need specify model_fun pred_fun. model_fun take single argument, data, return object can used make predictions pred_fun. ... also added argument allow optional arguments passed validate (see vignette(\"pminternal-examples\") examples user-defined functions take optional arguments). lasso_fun formats data glmnet, selects hyperparameter, lambda (controls degree regularization), via 10-fold cross-validation, fits final model ‘best’ value lambda returns. pred_fun take two arguments, model data, well optional argument(s) .... pred_fun work model object returned model_fun. glmnet objects predict method function lasso_predict simply formats data returns predictions. predict.glmnet returns matrix select first column return vector predicted risks. recommend use :: refer functions particular packages want run bootstrapping parallel. cores = 1 (cores argument supplied) cross-validation issue. code tests functions gusto. work intended can pass functions validate follows. using cross-validation estimate optimism. Note 10-fold cross-validation select best value lambda (.e., hyperparameter tuning) done fold performed validate.  examples user defined model functions (including elastic net random forest) can found vignette(\"validate-examples\").","code":"#library(glmnet)  lasso_fun <- function(data, ...){   y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      cv <- glmnet::cv.glmnet(x=x, y=y, alpha=1, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet::glmnet(x=x, y=y, alpha = 1, lambda = lambda, family=\"binomial\") }  lasso_predict <- function(model, data, ...){   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      plogis(glmnet::predict.glmnet(model, newx = x)[,1]) } lasso_app <- lasso_fun(gusto) lasso_p <- lasso_predict(model = lasso_app, data = gusto) # for calibration plot eval <- seq(min(lasso_p), max(lasso_p), length.out=100)  iv_lasso <- validate(method = \"cv_optimism\", data = gusto,                       outcome = \"y\", model_fun = lasso_fun,                       pred_fun = lasso_predict, B = 10,                       calib_args=list(eval=eval))  iv_lasso #>           apparent optimism corrected  n #> C           0.7982  0.00053    0.7977 10 #> Brier       0.0603  0.00000    0.0603 10 #> Intercept   0.0362  0.00898    0.0272 10 #> Slope       1.0173 -0.00657    1.0239 10 #> Eavg        0.0023 -0.00977    0.0121 10 #> E50         0.0019 -0.00567    0.0075 10 #> E90         0.0047 -0.02296    0.0276 10 #> Emax        0.0335 -0.05879    0.0923 10 #> ECI         0.0011 -0.04708    0.0482 10  cal_plot(iv_lasso)"},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"user-defined-score-functions","dir":"Articles","previous_headings":"","what":"User-defined score functions","title":"Getting started with `pminternal`","text":"scores returned score_binary enough clinical prediction model applications sometimes different measures may desired. can achieved specifying score_fun. take two arguments, y p, can take optional arguments. score_fun return named vector scores calculated y p. function sens_spec takes optional argument threshold used calculate sensitivity specificity. threshold specified set 0.5. call validate uses glm fit beginning vignette uses sens_spec function calculate bias-corrected sensitivity specificity threshold 0.2 (case assessing classification stability important).","code":"sens_spec <- function(y, p, ...){   # this function supports an optional   # arg: threshold (set to .5 if not specified)   dots <- list(...)   if (\"threshold\" %in% names(dots)){     thresh <- dots[[\"threshold\"]]   } else{     thresh <- .5   }   # make sure y is 1/0   if (is.logical(y)) y <- as.numeric(y)   # predicted 'class'   pcla <- as.numeric(p > thresh)     sens <- sum(y==1 & pcla==1)/sum(y==1)   spec <- sum(y==0 & pcla==0)/sum(y==0)    scores <- c(sens, spec)   names(scores) <- c(\"Sensitivity\", \"Specificity\")      return(scores) } validate(fit = mod, score_fun = sens_spec, threshold=.2,          method = \"cv_optimism\", B = 10) #>             apparent optimism corrected  n #> Sensitivity     0.31 0.003943      0.31 10 #> Specificity     0.94 0.000015      0.94 10"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"backward-selection","dir":"Articles","previous_headings":"","what":"Backward Selection","title":"More examples","text":"function implements glm variable selection via backward elimination using AIC. situation probably best stick lrm, fastbw, validate rms package (though note differences default step behavior) unless want additional calibration metrics offered pminternal want specify score function (see vignette(\"pminternal\")).","code":"stepglm <- function(data, ...){   m <- glm(y~., data=data, family=\"binomial\")   step(m, trace = 0) }  steppred <- function(model, data, ...){   predict(model, newdata = data, type = \"response\") }  validate(data = gusto, outcome = \"y\", model_fun = stepglm,           pred_fun = steppred, method = \"cv_opt\", B = 10) #>           apparent optimism corrected  n #> C           0.7983  0.00056    0.7977 10 #> Brier       0.0603  0.00000    0.0603 10 #> Intercept   0.0000  0.00891   -0.0089 10 #> Slope       1.0000 -0.00654    1.0065 10 #> Eavg        0.0023 -0.00994    0.0122 10 #> E50         0.0014 -0.00577    0.0072 10 #> E90         0.0044 -0.02360    0.0280 10 #> Emax        0.0447 -0.05162    0.0963 10 #> ECI         0.0017 -0.04705    0.0488 10"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"ridge","dir":"Articles","previous_headings":"","what":"Ridge","title":"More examples","text":"vignette(\"pminternal\") gives example glm lasso (L1) penalization. simple modify implement ridge (L2) penalization setting alpha = 0. Rather two separate functions specify optional argument, alpha, supplied validate. argument isn’t supplied function defaults alpha = 0. chunk evaluated output printed.","code":"#library(glmnet)  ridge_fun <- function(data, ...){   y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      cv <- glmnet::cv.glmnet(x=x, y=y, alpha=0, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet::glmnet(x=x, y=y, alpha = 0, lambda = lambda, family=\"binomial\") }  ridge_predict <- function(model, data, ...){   # note this is identical to lasso_predict from \"pminternal\" vignette   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      plogis(glmnet::predict.glmnet(model, newx = x)[,1]) }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = ridge_fun,           pred_fun = ridge_predict, B = 10) #>           apparent optimism corrected  n #> C           0.7986   0.0006    0.7980 10 #> Brier       0.0603   0.0000    0.0603 10 #> Intercept   0.2049   0.0062    0.1987 10 #> Slope       1.0966  -0.0073    1.1039 10 #> Eavg        0.0052  -0.0080    0.0132 10 #> E50         0.0046  -0.0043    0.0089 10 #> E90         0.0115  -0.0172    0.0287 10 #> Emax        0.0157  -0.0879    0.1035 10 #> ECI         0.0040  -0.0581    0.0621 10  # the use of package::function in user defined functions  # is especially important if you want to run  # boot_* or .632 in parallel via cores argument  # e.g. # validate(method = \".632\", data = gusto,  #          outcome = \"y\", model_fun = ridge_fun,  #          pred_fun = ridge_predict, B = 100, cores = 4) lognet_fun <- function(data, ...){      dots <- list(...)   if (\"alpha\" %in% names(dots)){     alpha <- dots[[\"alpha\"]]   } else{     alpha <- 0   }    y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      cv <- glmnet::cv.glmnet(x=x, y=y, alpha = alpha, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet::glmnet(x=x, y=y, alpha = alpha, lambda = lambda, family=\"binomial\") }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = lognet_fun,           pred_fun = ridge_predict, B = 10, alpha = 0.5)"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"elastic-net","dir":"Articles","previous_headings":"","what":"Elastic Net","title":"More examples","text":"implement model elastic net penalty need add steps select alpha. function evaluates nalpha equally spaced values alpha 0 1 (inclusive) selects values lambda alpha result minimum CV binomial deviance (changed via type.measure). nalpha optional argument. Note don’t need new predict function ridge_predict used. save build time chunk evaluated.","code":"enet_fun <- function(data, ...){      dots <- list(...)   if (\"nalpha\" %in% names(dots)){     nalpha <- dots[[\"nalpha\"]]   } else{     nalpha <- 21 # 0 to 1 in steps of 0.05   }      y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      # run 10 fold CV for each alpha   alphas <- seq(0, 1, length.out = nalpha)   res <- lapply(alphas, function(a){     cv <- glmnet::cv.glmnet(x=x, y=y, alpha = a, nfolds = 10, family=\"binomial\")     list(lambda = cv$lambda.min, bin.dev = min(cv$cvm))   })   # select result with min binomial deviance   j <- which.min(sapply(res, function(x) x$bin.dev))   # produce 'final' model with alpha and lambda   glmnet::glmnet(x=x, y=y, alpha = alphas[j], lambda = res[[j]][[\"lambda\"]], family=\"binomial\") }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = enet_fun,           pred_fun = ridge_predict, B = 10)"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"random-forest","dir":"Articles","previous_headings":"","what":"Random Forest","title":"More examples","text":"example use ranger package create model_fun allow optional arguments num.trees, max.depth, min.node.size; others added (see ?ranger).","code":"rf_fun <- function(data, ...){      dots <- list(...)   num.trees <- if (\"num.trees\" %in% names(dots)) dots[[\"num.trees\"]] else 500   max.depth <- if (\"max.depth\" %in% names(dots)) dots[[\"max.depth\"]] else NULL   min.node.size <- if (\"min.node.size\" %in% names(dots)) dots[[\"min.node.size\"]] else 1      # best to make sure y is a factor where '1' is level 2   data$y <- factor(data$y, levels = 0:1)      ranger::ranger(y~., data = data, probability = TRUE,                   num.trees = num.trees,                   max.depth = max.depth,                   min.node.size = min.node.size) }  rf_predict <- function(model, data, ...){   predict(model, data = data)$predictions[, 2]  }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = rf_fun,           pred_fun = rf_predict, B = 10) #>           apparent optimism corrected  n #> C            0.970  0.00057     0.969 10 #> Brier        0.044  0.00000     0.044 10 #> Intercept    3.838 -0.14053     3.979 10 #> Slope        3.246 -0.09513     3.341 10 #> Eavg         0.057 -0.00395     0.061 10 #> E50          0.031 -0.00260     0.034 10 #> E90          0.067 -0.02725     0.094 10 #> Emax         0.555  0.03954     0.515 10 #> ECI          1.196 -0.08622     1.283 10  # instead of unlimited tree depth... validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = rf_fun,           pred_fun = rf_predict, B = 10, max.depth = 3) #>           apparent optimism corrected  n #> C            0.821  0.00096     0.820 10 #> Brier        0.061  0.00000     0.061 10 #> Intercept    2.276  0.00267     2.273 10 #> Slope        2.026 -0.00982     2.035 10 #> Eavg         0.031 -0.00352     0.035 10 #> E50          0.023 -0.00113     0.024 10 #> E90          0.054 -0.00468     0.059 10 #> Emax         0.390  0.06376     0.326 10 #> ECI          0.229 -0.09231     0.321 10"},{"path":"https://stephenrho.github.io/pminternal/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Stephen Rhodes. Author, maintainer, copyright holder.","code":""},{"path":"https://stephenrho.github.io/pminternal/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Rhodes S (2025). pminternal: Internal validation clinical prediction models. R package version 0.1.0, https://stephenrho.github.io/pminternal/.","code":"@Manual{,   title = {pminternal: Internal validation of clinical prediction models},   author = {Stephen Rhodes},   year = {2025},   note = {R package version 0.1.0},   url = {https://stephenrho.github.io/pminternal/}, }"},{"path":"https://stephenrho.github.io/pminternal/index.html","id":"pminternal-internal-validation-of-clinical-prediction-models","dir":"","previous_headings":"","what":"Internal Validation of Clinical Prediction Models","title":"Internal Validation of Clinical Prediction Models","text":"goal offer package can produce bias-corrected performance measures clinical prediction models binary outcomes range model development approaches available R (similar rms::validate). also functions assessing prediction stability, described Riley Collins (2023). install:","code":"install.packages(\"pminternal\") # cran # or  devtools::install_github(\"stephenrho/pminternal\") # development"},{"path":"https://stephenrho.github.io/pminternal/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Internal Validation of Clinical Prediction Models","text":"example use bootstrapping correct performance measures glm via calculation ‘optimism’ (see vignette(\"pminternal\"), vignette(\"validate-examples\"), vignette(\"missing-data\") examples): available methods calculating bias corrected performance simple bootstrap (boot_simple), 0.632 bootstrap optimism (.632), optimism via cross-validation (cv_optimism), regular cross-validation (cv_average). Please see ?pminternal::validate references therein. Bias corrected calibration curves can also produced (see pminternal::cal_plot). Confidence intervals can also added via confint. models supported via fit, users able specify model (model_fun) prediction (pred_fun) functions shown . Note specifying user-defined model prediction functions data outcome must also provided. crucial model_fun implements entire model development procedure (variable selection, hyperparameter tuning, etc). examples, see vignette(\"pminternal\") vignette(\"validate-examples\"). output validate (method = \"boot_*\") can used produce plots assessing stability model predictions (across models developed bootstrap resamples). prediction ()stability plot shows predictions B (case 100) bootstrap models applied development data.  MAPE plot shows mean absolute prediction error, difference predicted risk development model B bootstrap models.  calibration ()stability plot depict original calibration curve along B calibration curves bootstrap models applied original data (y).  classification instability index (CII) proportion individuals change predicted class (present/absent, 1/0) predicted risk compared threshold. example, patient predicted class 1 receive CII 0.3 30% bootstrap models led predicted class 0.  Decision curves implied original bootstrap models can also plotted.","code":"library(pminternal)  # make some data set.seed(2345) n <- 800 p <- 10  X <- matrix(rnorm(n*p), nrow = n, ncol = p) LP <- -1 + apply(X[, 1:5], 1, sum) # first 5 variables predict outcome y <- rbinom(n, 1, plogis(LP))  dat <- data.frame(y, X)  # fit a model mod <- glm(y ~ ., data = dat, family = \"binomial\")  # calculate bootstrap optimism corrected performance measures (val <- validate(fit = mod, method = \"boot_optimism\", B = 100)) #> It is recommended that B >= 200 for bootstrap validation #>           apparent optimism corrected   n #> C           0.8567   0.0093    0.8474 100 #> Brier       0.1423  -0.0054    0.1477 100 #> Intercept   0.0000   0.0175   -0.0175 100 #> Slope       1.0000   0.0529    0.9471 100 #> Eavg        0.0045  -0.0048    0.0093 100 #> E50         0.0039  -0.0050    0.0089 100 #> E90         0.0081  -0.0107    0.0187 100 #> Emax        0.0109  -0.0057    0.0165 100 #> ECI         0.0027  -0.0038    0.0065 100 # fit a glm with lasso penalty library(glmnet) #> Loading required package: Matrix #> Loaded glmnet 4.1-8  lasso_fun <- function(data, ...){   y <- data$y   x <- as.matrix(data[, which(colnames(data) != \"y\")])      cv <- cv.glmnet(x=x, y=y, alpha=1, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet(x=x, y=y, alpha = 1, lambda = lambda, family=\"binomial\") }  lasso_predict <- function(model, data, ...){   y <- data$y   x <- as.matrix(data[, which(colnames(data) != \"y\")])      predict(model, newx = x, type = \"response\")[,1] }  (val <- validate(data = dat, outcome = \"y\",                   model_fun = lasso_fun, pred_fun = lasso_predict,                   method = \"boot_optimism\", B = 100)) #> It is recommended that B >= 200 for bootstrap validation #>           apparent optimism corrected   n #> C            0.856   0.0070     0.849 100 #> Brier        0.143  -0.0041     0.147 100 #> Intercept    0.080   0.0191     0.061 100 #> Slope        1.155   0.0449     1.110 100 #> Eavg         0.020   0.0013     0.019 100 #> E50          0.019   0.0026     0.017 100 #> E90          0.040   0.0021     0.038 100 #> Emax         0.044   0.0145     0.029 100 #> ECI          0.053   0.0087     0.044 100 prediction_stability(val, smooth_bounds = TRUE) mape_stability(val) calibration_stability(val) classification_stability(val, threshold = .4) dcurve_stability(val)"},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"Estimate bias-corrected scores via calculation bootstrap optimism (standard .632). Can also produce estimates assessing stability prediction model predictions. function called validate.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"","code":"boot_optimism(   data,   outcome,   model_fun,   pred_fun,   score_fun,   method = c(\"boot\", \".632\"),   B = 200,   ... )"},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"data data used developing model. contain variables considered (.e., even excluded variable selection development sample) outcome character denoting column name outcome data. model_fun function takes least one argument, data. function implement entire model development procedure (.e., hyperparameter tuning, variable selection, imputation). Additional arguments can provided via .... function return object works pred_fun. pred_fun function takes least two arguments, model data. function return numeric vector predicted probabilities outcome length number rows data important take account missing data treated (e.g., predict.glm omits predictions rows missing values). see vignette(\"missing-data\", package = \"pminternal\"). score_fun function calculate metrics interest. specified score_binary used. method \"boot\" \".632\". former estimates bootstrap optimism score subtracts apparent scores (simple bootstrap estimates also produced -product). latter estimates \".632\" optimism described Harrell (2015). See validate details. B number bootstrap resamples run ... additional arguments model_fun, pred_fun, /score_fun.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"list class internal_boot containing: apparent - scores calculated original data using original model. optimism - estimates optimism score (average difference score bootstrap models evaluated bootstrap vs original sample) can subtracted 'apparent' performance calculated using original model original data. corrected - 'bias corrected' scores (apparent - optimism) simple - method = \"boot\", estimates scores derived 'simple bootstrap'. average score calculated bootstrap models evaluated original outcome data. NULL method = \".632\" stability - method = \"boot\", N,(B+1) matrix N number observations data B number bootstrap samples. first column contains original predictions subsequent B columns contain predicted probabilities outcome bootstrap model evaluated original data. may fewer B+1 columns errors occur resamples (model_fun throws error scores NA). NULL method = \".632\"","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation predictive models: efficiency procedures logistic regression analysis. Journal clinical epidemiology, 54(8), 774-781. Harrell Jr F. E. (2015). Regression Modeling Strategies: applications linear models, logistic ordinal regression, survival analysis. New York: Springer Science, LLC.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 1000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.186 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model model_fun <- function(data, ...){   glm(y ~ x1 + x2, data=data, family=\"binomial\") }  pred_fun <- function(model, data, ...){   predict(model, newdata=data, type=\"response\") }  boot_optimism(data=dat, outcome=\"y\", model_fun=model_fun, pred_fun=pred_fun,               method=\"boot\", B=20) # B set to 20 for example but should be >= 200 #>                          C   Brier Intercept  Slope    Eavg     E50     E90 #> Apparent            0.7964  0.1262   6.6e-15  1.000  0.0195  0.0162  0.0328 #> Optimism            0.0049 -0.0018   1.0e-02  0.016  0.0044  0.0051  0.0076 #> B Optimism         20.0000 20.0000   2.0e+01 20.000 20.0000 20.0000 20.0000 #> Optimism Corrected  0.7915  0.1280  -1.0e-02  0.984  0.0151  0.0111  0.0251 #> Simple Corrected    0.7953  0.1268  -1.0e-02  0.984  0.0222  0.0157  0.0458 #> B Simple           20.0000 20.0000   2.0e+01 20.000 20.0000 20.0000 20.0000 #>                      Emax    ECI #> Apparent            0.101  0.062 #> Optimism            0.020  0.048 #> B Optimism         20.000 20.000 #> Optimism Corrected  0.081  0.014 #> Simple Corrected    0.109  0.099 #> B Simple           20.000 20.000"},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default settings for calibration curves — cal_defaults","title":"Get default settings for calibration curves — cal_defaults","text":"Get default settings calibration curves","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default settings for calibration curves — cal_defaults","text":"","code":"cal_defaults(x = NULL)"},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default settings for calibration curves — cal_defaults","text":"x ignored","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default settings for calibration curves — cal_defaults","text":"list containing default arguments supply pmcalibration::pmcalibration","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot apparent and bias-corrected calibration curves — cal_plot","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"Plot apparent bias-corrected calibration curves","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"","code":"cal_plot(   x,   xlim,   ylim,   xlab,   ylab,   app_col,   bc_col,   app_lty,   bc_lty,   plotci = c(\"if\", \"yes\", \"no\") )"},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"x object returned validate. Original call specified 'eval' argument. See score_binary. xlim x limits (default = c(0, max either curve)) ylim y limits (default = c(0, max either curve)) xlab title x axis ylab title y axis app_col color apparent calibration curve (default = 'black') bc_col color bias-corrected calibration curve (default = 'black') app_lty line type apparent calibration curve (default = 1) bc_lty line type bias-corrected calibration curve (default = 2) plotci plot confidence intervals ('yes') (''). 'yes' x confidence intervals added confint.internal_validate. '' (default) plots CIs available.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"plots apparent bias-corrected curves. Silently returns data.frame can used produce 'publication ready' plot. Columns follows: predicted = values x-axis, apparent = value apparent curve, bias_corrected = value bias-corrected curve. Confidence intervals included available.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ x1 + x2, data=dat, family=\"binomial\")  # to get a plot of bias-corrected calibration we need # to specify 'eval' argument via 'calib_args' # this argument specifies at what points to evalulate the # calibration curve for plotting. The example below uses # 100 equally spaced points between the min and max # original prediction.  p <- predict(m1, type=\"response\") p100 <- seq(min(p), max(p), length.out=100)  m1_iv <- validate(m1, method=\"cv_optimism\", B=10,                   calib_args = list(eval=p100)) # calib_ags can be used to set other calibration curve # settings: see pmcalibration::pmcalibration  cal_plot(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot calibration stability across bootstrap replicates — calibration_stability","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"calibration ()stability plot shows calibration curves bootstrap models evaluated original outcome. stable model produce boot calibration curves differ minimally 'apparent' curve.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"","code":"calibration_stability(   x,   calib_args,   xlim,   ylim,   xlab,   ylab,   col,   subset,   plot = TRUE )"},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"x object produced validate method = \"boot_*\" (boot_optimism method=\"boot\") calib_args settings calibration curve (see pmcalibration::pmcalibration). unspecified settings given cal_defaults 'eval' set 100 (evaluate curve 100 points min max prediction). xlim x limits (default = c(0,1)) ylim y limits (default = c(0,1)) xlab title x axis ylab title y axis col color lines bootstrap models (default = grDevices::grey(.5, .3)) subset vector observations include (row indices). dataset large fitting B curves demanding. can used select random subset observations. plot FALSE just returns curves (see value)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"plots calibration ()stability. Invisibly returns list containing data curve (p=x-axis, pc=y-axis). first element list apparent curve (original model original outcome).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"Riley, R. D., & Collins, G. S. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"","code":"# \\donttest{ set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  calibration_stability(m1_iv)  # }"},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification instability plot — classification_stability","title":"Classification instability plot — classification_stability","text":"Classification instability plot shows relationship original model estimated risk classification instability index (CII). CII proportion bootstrap replicates predicted class (0 p <= threshold; 1 p > threshold) different obtained original model. risk predictions around threshold exhibit elevated CII unstable model exhibit high CII across range risk predictions. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification instability plot — classification_stability","text":"","code":"classification_stability(   x,   threshold,   xlim,   ylim,   xlab,   ylab,   pch,   cex,   col,   subset,   plot = TRUE )"},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification instability plot — classification_stability","text":"x object produced validate method = \"boot_*\" (boot_optimism method=\"boot\") threshold estimated risks threshold get predicted 'class' 1, otherwise 0. xlim x limits (default = range estimated risks) ylim y limits (default = c(0, maximum CII)) xlab title x axis ylab title y axis pch plotting character (default = 16) cex controls point size (default = 1) col color points (default = grDevices::grey(.5, .5)) subset vector observations include (row indices). can used select random subset observations. plot FALSE just returns CII values (see value)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classification instability plot — classification_stability","text":"plots classification ()stability. Invisibly returns estimates CII observation.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Classification instability plot — classification_stability","text":"Riley, R. D., & Collins, G. S. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classification instability plot — classification_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  classification_stability(m1_iv, threshold=.2)"},{"path":"https://stephenrho.github.io/pminternal/reference/confint.internal_validatesummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","title":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","text":"Implements methods discussed Noma et al. (2021), plus others tested. Specifically, Noma et al. discuss bootstrap optimism correction (\"boot_optimism\" \".632\") percentile bootstrap (ci_type = \"perc\"). paper contains simulation results coverage properties CIs. used validate something bootstrap optimism correction request normal approximation CIs please note approaches (knowledge) thoroughly tested. ci_type = \"norm\" included might able reduce number runs needed \"twostage\" CIs. See details difference \"shifted\" \"twostage\". \"norm\" CIs likely perform poorly performance measures, calibration Intercept Slope, regular glms always 0 1, respectively, assessment apparent performance. \"shifted\" CIs based apparent performance meaningless measures. Use untested methods caution!","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/confint.internal_validatesummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","text":"","code":"# S3 method for class 'internal_validate' confint(   object,   parm,   level = 0.95,   method = c(\"shifted\", \"twostage\"),   ci_type = c(\"perc\", \"norm\"),   R = 1000,   add = TRUE,   ... )"},{"path":"https://stephenrho.github.io/pminternal/reference/confint.internal_validatesummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","text":"object created call validate parm specification performance measures given confidence intervals, either vector numbers vector names. missing, scores considered. level confidence level required method \"shifted\" \"twostage\" (see details) ci_type percentile (\"perc\") normal approximation (\"norm\") bootstrap CIs R number replicates add return object additional slot containing CIs (default) just return CIs ... additional arguments (currently ignored)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/confint.internal_validatesummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","text":"list two elements, matrix columns giving lower upper confidence limits measure. One apparent one bias-corrected measures. Columns labelled (1-level)/2 1 - (1-level)/2 % (default 2.5% 97.5%).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/confint.internal_validatesummary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","text":"two methods follows (see Noma et al. (2021) details): shifted (default) approach based shifting bootstrap CIs apparent performance optimism. makes faster option calculation apparent performance needed replicate. CI apparent performance [lower, upper], resulting CI bias-corrected performance [lower - optimism, upper - optimism]. Note method available using optimism based approach (\"cv_optimism\" untested Noma et al). twostage approach creates bootstrap resample data runs entire validation procedure resample (number 'inner' replicates, determined B original validate call). CI constructed using corrected estimates R 'outer' replicates. involves R*B replicates, take long time. Note validate takes cores argument can allow inner samples run parallel.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/confint.internal_validatesummary.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Confidence intervals for bias-corrected performance measures — confint.internal_validate","text":"Noma, H., Shinozaki, T., Iba, K., Teramukai, S., & Furukawa, T. . (2021). Confidence intervals prediction accuracy measures multivariable prediction models based bootstrap‐based optimism correction methods. Statistics Medicine, 40(26), 5691-5701.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate bias-corrected scores via cross-validation — crossval","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"Estimate bias-corrected scores via cross-validation. CV used calculate optimism subtracted apparent scores calculate average performance sample (held ) data. function called validate.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"","code":"crossval(data, outcome, model_fun, pred_fun, score_fun, k = 10, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"data data used developing model. contain variables considered (.e., even excluded variable selection development sample) outcome character denoting column name outcome data. model_fun function takes least one argument, data. function implement entire model development procedure (.e., hyperparameter tuning, variable selection, imputation). Additional arguments can provided via .... function return object works pred_fun. pred_fun function takes least two arguments, model data. function return numeric vector predicted probabilities outcome length number rows data important take account missing data treated (e.g., predict.glm omits predictions rows missing values). score_fun function calculate metrics interest. specified score_binary used. k number folds. Typically scores need greater 2 observations calculated folds chosen mind. ... additional arguments model_fun, pred_fun, /score_fun.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"list class internal_cv containing: apparent - scores calculated original data using original model. optimism - estimates optimism score (average difference score training data vs test data fold) can subtracted 'apparent' performance calculated using original model original data. cv_optimism_corrected - 'bias corrected' scores (apparent - optimism). produced rms::validate, rms::predab.resample. cv_average - average scores calculated test (held ) data. metric described Steyerberg et al. (2001). indices - indices used define test set fold.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation predictive models: efficiency procedures logistic regression analysis. Journal clinical epidemiology, 54(8), 774-781.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 1000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.186 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model #m1 <- glm(y ~ x1 + x2, data=dat, family=\"binomial\")  model_fun <- function(data, ...){   glm(y ~ x1 + x2, data=data, family=\"binomial\") }  pred_fun <- function(model, data, ...){   predict(model, newdata=data, type=\"response\") }  # CV Corrected = Apparent - CV Optimism # CV Average = average score in held out fold crossval(data=dat, outcome=\"y\", model_fun=model_fun, pred_fun=pred_fun, k=10) #>                          C    Brier Intercept Slope   Eavg    E50    E90  Emax #> Apparent            0.7964  1.3e-01   6.6e-15  1.00  0.020  0.016  0.033  0.10 #> CV Optimism        -0.0049 -4.2e-18   1.6e-02 -0.04 -0.033 -0.027 -0.069 -0.10 #> B Optimism         10.0000  1.0e+01   1.0e+01 10.00 10.000 10.000 10.000 10.00 #> Optimism Corrected  0.8012  1.3e-01  -1.6e-02  1.04  0.053  0.044  0.102  0.21 #> CV Average          0.8013  1.3e-01  -1.7e-02  1.04  0.053  0.044  0.102  0.21 #> B Average          10.0000  1.0e+01   1.0e+01 10.00 10.000 10.000 10.000 10.00 #>                       ECI #> Apparent            0.062 #> CV Optimism        -0.489 #> B Optimism         10.000 #> Optimism Corrected  0.550 #> CV Average          0.556 #> B Average          10.000"},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot decision curve stability across bootstrap replicates — dcurve_stability","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"decision curve ()stability plot shows decision curves bootstrap models evaluated original outcome. stable model produce curves differ minimally 'apparent' curve. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"","code":"dcurve_stability(   x,   thresholds = seq(0, 0.99, by = 0.01),   xlim,   ylim,   xlab,   ylab,   col,   subset,   plot = TRUE )"},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"x object produced validate method = \"boot_*\" (boot_optimism method=\"boot\") thresholds points evaluate decision curves (see dcurves::dca) xlim x limits (default = range thresholds) ylim y limits (default = range net benefit) xlab title x axis ylab title y axis col color points (default = grDevices::grey(.5, .5)) subset vector observations include (row indices). can used select random subset observations. plot FALSE just returns curves (see value)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"plots decision curve ()stability. Invisibly returns list containing data curve. returned dcurves::dca. first element list apparent curve (original model original outcome).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"Riley, R. D., & Collins, G. S. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"","code":"# \\donttest{ set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  dcurve_stability(m1_iv)  # }"},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Get stability from internal_validate or internal_boot object — get_stability","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"Get stability internal_validate internal_boot object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"","code":"get_stability(x)"},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"x internal_validate (see validate) internal_boot (see boot_optimism) object. former created method = \"boot_optimism\" \"boot_simple\".","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"list containing stability matrix (n x B+1 matrix. column contains predictions p observations. First column contains predictions original model. columns bootstrap models) original binary outcome (y). Unsuccessful resamples omitted.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean absolute predictor error (MAPE) stability plot — mape_stability","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"MAPE ()stability plot shows mean absolute predictor error (average absolute difference original estimated risk risk B bootstrap models) function apparent estimated risk (prediction original/development model). See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"","code":"mape_stability(x, xlim, ylim, xlab, ylab, pch, cex, col, subset, plot = TRUE)"},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"x object produced validate method = \"boot_*\" (boot_optimism method=\"boot\") xlim x limits (default = range estimated risks) ylim y limits (default = c(0, maximum mape)) xlab title x axis ylab title y axis pch plotting character (default = 16) cex controls point size (default = 1) col color points (default = grDevices::grey(.5, .5)) subset vector observations include (row indices). can used select random subset observations. plot FALSE just returns MAPE values (see value)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"plots calibration ()stability. Invisibly returns list containing individual average MAPE.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"Riley, R. D., & Collins, G. S. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  mape_stability(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/pminternal-package.html","id":null,"dir":"Reference","previous_headings":"","what":"pminternal: Internal Validation of Clinical Prediction Models — pminternal-package","title":"pminternal: Internal Validation of Clinical Prediction Models — pminternal-package","text":"Conduct internal validation clinical prediction model binary outcome. Produce bias corrected performance metrics (c-statistic, Brier score, calibration intercept/slope) via bootstrap (simple bootstrap, bootstrap optimism, .632 optimism) cross-validation (CV optimism, CV average). Also includes functions assess model stability via bootstrap resampling. See Steyerberg et al. (2001) doi:10.1016/s0895-4356(01)00341-9 ; Harrell (2015) doi:10.1007/978-3-319-19425-7 ; Riley Collins (2023) doi:10.1002/bimj.202200302 .","code":""},{"path":[]},{"path":"https://stephenrho.github.io/pminternal/reference/pminternal-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pminternal: Internal Validation of Clinical Prediction Models — pminternal-package","text":"Maintainer: Stephen Rhodes steverho89@gmail.com [copyright holder]","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot prediction stability across bootstrap replicates — prediction_stability","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"prediction ()stability plot shows estimated risk probabilities models developed resampled data evaluated original development data function 'apparent' prediction (prediction original/development model evaluated original data). stable model produce points exhibit minimal dispersion. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"","code":"prediction_stability(   x,   bounds = 0.95,   smooth_bounds = FALSE,   xlab,   ylab,   pch,   cex,   col,   lty,   span,   subset,   plot = TRUE )"},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"x object produced validate method = \"boot_*\" (boot_optimism method=\"boot\") bounds width 'stability interval' (percentiles bootstrap model predictions). NULL = add bounds plot. smooth_bounds TRUE, use loess smooth bounds (default = FALSE) xlab title x axis ylab title y axis pch plotting character (default = 16) cex controls point size (default = 0.4) col color points (default = grDevices::grey(.5, .5)) lty line type bounds (default = 2) span controls degree smoothing (see loess; default = 0.75) subset vector observations include (row indices). dataset large plotting N points B bootstrap resamples demanding. can used select random subset observations. plot FALSE just returns stability matrix","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"plots prediction ()stability. stability bounds smoothed. Invisibly returns stability matrix (column 1 original predictions) can used creating plots packages/software.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"Riley, R. D., & Collins, G. S. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  prediction_stability(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a internal_boot object — print.internal_boot","title":"Print a internal_boot object — print.internal_boot","text":"Print internal_boot object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a internal_boot object — print.internal_boot","text":"","code":"# S3 method for class 'internal_boot' print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a internal_boot object — print.internal_boot","text":"x object created boot_optimism digits number digits print (default = 2) ... additional arguments print","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a internal_boot object — print.internal_boot","text":"invisibly returns x prints estimates console","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a internal_cv object — print.internal_cv","title":"Print a internal_cv object — print.internal_cv","text":"Print internal_cv object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a internal_cv object — print.internal_cv","text":"","code":"# S3 method for class 'internal_cv' print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a internal_cv object — print.internal_cv","text":"x object created crossval digits number digits print (default = 2) ... additional arguments print","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a internal_cv object — print.internal_cv","text":"invisibly returns x prints estimates console","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"print a internal_validate object — print.internal_validate","title":"print a internal_validate object — print.internal_validate","text":"print internal_validate object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"print a internal_validate object — print.internal_validate","text":"","code":"# S3 method for class 'internal_validate' print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"print a internal_validate object — print.internal_validate","text":"x internal_validate object digits number digits print ... optional arguments passed print","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"print a internal_validate object — print.internal_validate","text":"prints summary","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Print summary of internal_validate object — print.internal_validatesummary","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"Print summary internal_validate object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"","code":"# S3 method for class 'internal_validatesummary' print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"x internal_validatesummary object digits number digits print ... ignored","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"invisible(x) - prints summary","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Score predictions for binary events — score_binary","title":"Score predictions for binary events — score_binary","text":"Calculate scores summarizing discrimination/calibration predictions observed binary events. score_fun defined calling validate function used.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Score predictions for binary events — score_binary","text":"","code":"score_binary(y, p, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Score predictions for binary events — score_binary","text":"y vector containing binary outcome p vector predictions ... additional arguments. function supports calib_args optional argument. calib_args contain arguments pmcalibration::pmcalibration. calibration plot (apparent vs bias corrected calibration curves via cal_plot) desired argument 'eval' provided. points evaluate calibration curve boot resample crossvalidation fold. good option calib_args = list(eval = seq(min(p), max(p), length.=100)); p predictions original model evaluated original data. Dots can used supply additional arguments user-defined functions.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Score predictions for binary events — score_binary","text":"named vector scores (see Details)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Score predictions for binary events — score_binary","text":"following measures returned named vector. C c-statistic (aka area ROC curve). Probability randomly selected observation y = 1 higher p compared randomly selected y = 0. Brier mean squared error - mean((y - p)^2) Intercept Intercept logistic calibration model: glm(y ~ 1 + offset(qlogis(p)), family=\"binomial\") Slope Slope logistic calibration model: glm(y ~ 1 + qlogis(p), family=\"binomial\") Eavg average absolute difference p calibration curve (aka integrated calibration index ICI). E50 median absolute difference p calibration curve E90 90th percentile absolute difference p calibration curve Emax maximum absolute difference p calibration curve ECI average squared difference p calibration curve. Estimated calibration index (Van Hoorde et al. 2015) cal_plot eval specified (via calib_args), values plotting apparent bias-corrected calibration curves returned (see cal_plot). default omitted summary printed (see summary.internal_validate). Logistic calibration calibration metrics non-linear calibration curves assessing 'moderate-calibration' (Eavg, E50, E90, Emax, ECI; see references) calculated via pmcalibration package. default settings can modified passing calib_args validate call. calib_args named list corresponding arguments pmcalibration::pmcalibration.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Score predictions for binary events — score_binary","text":"Austin PC, Steyerberg EW. (2019) Integrated Calibration Index (ICI) related metrics quantifying calibration logistic regression models. Statistics Medicine. 38, pp. 1–15. https://doi.org/10.1002/sim.8281 Van Hoorde, K., Van Huffel, S., Timmerman, D., Bourne, T., Van Calster, B. (2015). spline-based tool assess visualize calibration multiclass risk predictions. Journal Biomedical Informatics, 54, pp. 283-93 Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina M., Steyerberg E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Score predictions for binary events — score_binary","text":"","code":"p <- runif(100) y <- rbinom(length(p), 1, p) score_binary(y = y, p = p) #>          C      Brier  Intercept      Slope       Eavg        E50        E90  #> 0.83313325 0.16627180 0.10230809 1.09364881 0.01892466 0.02033055 0.03572761  #>       Emax        ECI  #> 0.03680029 0.05172805"},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a internal_validate object — summary.internal_validate","title":"Summarize a internal_validate object — summary.internal_validate","text":"Summarize internal_validate object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a internal_validate object — summary.internal_validate","text":"","code":"# S3 method for class 'internal_validate' summary(object, ignore_scores = \"^cal_plot\", ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a internal_validate object — summary.internal_validate","text":"object created call validate ignore_scores string used identify scores omit summary. score_binary produces scores prefix 'cal_plot' calibration plot desired (see cal_plot) ignored default. ... ignored","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a internal_validate object — summary.internal_validate","text":"data.frame 4 columns (apparent score, optimism, bias-corrected score, number successful resamples/folds) one row per score. methods produce optimism estimate row may NA. confidence intervals added measures via confint.internal_validate, two additional columns containing lower upper bounds bias-corrected performance.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize a internal_validate object — summary.internal_validate","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation summary(m1_iv) #>           apparent optimism corrected  n #> C           0.7779  0.00158    0.7764 10 #> Brier       0.1335 -0.00111    0.1346 10 #> Intercept   0.0000 -0.01917    0.0192 10 #> Slope       1.0000  0.00083    0.9992 10 #> Eavg        0.0076  0.00516    0.0024 10 #> E50         0.0064  0.00381    0.0026 10 #> E90         0.0115  0.00882    0.0027 10 #> Emax        0.0580  0.07771   -0.0197 10 #> ECI         0.0110  0.03656   -0.0256 10"},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":null,"dir":"Reference","previous_headings":"","what":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"Performs internal validation prediction model development procedure via bootstrapping cross-validation. Many model types supported via insight marginaleffects packages users can supply user-defined functions implement model development procedure retrieve predictions. Bias-corrected scores estimates optimism (applicable) provided. See confint.internal_validate calculation confidence intervals.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"","code":"validate(   fit,   method = c(\"boot_optimism\", \"boot_simple\", \".632\", \"cv_optimism\", \"cv_average\", \"none\"),   data,   outcome,   model_fun,   pred_fun,   score_fun,   B,   ... )"},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"fit model object. fit given insight package used extract data, outcome, original model call. Therefore, important fit supported insight implements entire model development process (see Harrell 2015). fit given selection variables method give accurate bias-correction. Model predictions obtained via marginaleffects::get_predict type = \"response\" fit compatible function. fit provided arguments data, outcome, model_fun, pred_fun ignored. method bias-correction method. Valid options \"boot_optimism\", \"boot_simple\", \".632\", \"cv_optimism\", \"cv_average\", \"none\" (return apparent performance). See details. data data.frame containing data used fit development model outcome character denoting column name outcome data model_fun models supplied via fit function takes one named argument: 'data' (function include ... among arguments). function implement entire model development procedure (hyperparameter tuning, variable selection, imputation etc) return object can used pred_fun. Additional arguments can supplied ... pred_fun models supplied via fit function takes two named arguments: 'model' 'data' (function include ... among arguments). 'model' object returned model_fun. function return vector predicted risk probabilities length number rows data. Additional arguments can supplied ... score_fun function used produce performance measures predicted risks observed binary outcome. take two named arguments: 'y' 'p' (function include ... among arguments). function return named vector scores. unspecified score_binary used good purposes. B number bootstrap replicates crossvalidation folds. unspecified B set 200 method = \"boot_*\"/\".632\", set 10 method = \"cv_*\". ... additional arguments user-defined functions. Arguments producing calibration curves can set via 'calib_args' named list (see cal_plot score_binary). method = \"boot_optimism\", \"boot_simple\", \".632\" users can specify cores argument (e.g., cores = 4) run bootstrap samples parallel.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"object class internal_validate containing apparent bias-corrected estimates performance scores. method = \"boot_*\" also contains results pertaining stability predictions across bootstrapped models (see Riley Collins, 2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"Internal validation can provide bias-corrected estimates performance (e.g., C-statistic/AUC, calibration intercept/slope) model development procedure (.e., expected performance procedure applied another sample size population; see references). several approaches producing bias-corrected estimates (see ). important fit model_fun provided implement entire model development procedure, including hyperparameter tuning /variable selection. Note validate little check missing values predictors/features. fit supplied insight::get_data extract data used fit model usually result complete cases used. User-defined model predict functions can specified handle missing values among predictor variables. Currently user supplied data rows missing outcome values removed. method Different options method argument described : boot_optimism (default) estimates optimism score subtracts apparent score (score calculated original/development model evaluated original sample). new model fit using procedure using bootstrap resample. Scores calculated applying boot model boot sample (\\(S_{boot}\\)) original sample (\\(S_{orig}\\)) difference gives estimate optimism given resample (\\(S_{boot} - S_{orig}\\)). average optimism across B resamples subtracted apparent score produce bias corrected score. boot_simple implements simple bootstrap. B bootstrap models fit evaluated original data. average score across B replicates bias-corrected score. .632 implements Harrell's adaption Efron's .632 estimator binary outcomes (see rms::predab.resample rms::validate). case estimate optimism \\(0.632 \\times (S_{app} - mean(S_{omit} \\times w))\\) \\(S_{app}\\) apparent performance score \\(S_{omit}\\) score estimated using bootstrap model evaluated --sample observations \\(w\\) weights proportion observations omitted (see Harrell 2015, p. 115). cv_optimism estimate optimism via B-fold crossvalidation. Optimism average difference performance measure predictions made training vs test (held fold) data. approach implemented rms::validate method=\"crossvalidation\". cv_average bias corrected scores average scores calculated assessing model developed fold evaluated test/held data. approach described compared \"boot_optimism\" \".632\" Steyerberg et al. (2001). Calibration curves make calibration curves calculate associated estimates (ICI, ECI, etc - see score_binary) validate uses default arguments cal_defaults. arguments passed pmcalibration package (see ?pmcalibration::pmcalibration options). calibration plot (apparent vs bias corrected calibration curves via cal_plot) desired, argument 'eval' provided. points evaluate calibration curve boot resample crossvalidation fold. good option calib_args = list(eval = seq(min(p), max(p), length.=100)); p predictions original model evaluated original data. Number resamples/folds less requested model_fun produces error score_binary supplied constant predictions outcomes (e.g. (y == 0)) returned scores NA. omitted calculation optimism bias-corrected estimates (cv_average, boot_simple) number successful resamples/folds < B. validate collects error messages produce warning summarizing . number successful samples given 'n' column printed summary 'internal_validate' object. important understand causing loss resamples/folds. potential sources (need added ) rare events resamples/folds may resulting samples zero outcomes. 'cv_*' especially case B (n folds) set high. may problems factor/binary predictor variables rare levels, dealt specifying model_fun omits variables model formula one level present. issue may related construction calibration curves may addressed carefully selecting settings (see section ).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation predictive models: efficiency procedures logistic regression analysis. Journal clinical epidemiology, 54(8), 774-781. Harrell Jr F. E. (2015). Regression Modeling Strategies: applications linear models, logistic ordinal regression, survival analysis. New York: Springer Science, LLC. Efron (1983). “Estimating error rate prediction rule: improvement cross-validation”. Journal American Statistical Association, 78(382):316-331 Van Calster, B., Steyerberg, E. W., Wynants, L., van Smeden, M. (2023). thing validated prediction model. BMC medicine, 21(1), 70. Riley, R. D., & Collins, G. S. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biometrical Journal, 65(8), 2200302. doi:10.1002/bimj.202200302","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation m1_iv #>           apparent optimism corrected  n #> C           0.7779  0.00158    0.7764 10 #> Brier       0.1335 -0.00111    0.1346 10 #> Intercept   0.0000 -0.01917    0.0192 10 #> Slope       1.0000  0.00083    0.9992 10 #> Eavg        0.0076  0.00516    0.0024 10 #> E50         0.0064  0.00381    0.0026 10 #> E90         0.0115  0.00882    0.0027 10 #> Emax        0.0580  0.07771   -0.0197 10 #> ECI         0.0110  0.03656   -0.0256 10"},{"path":"https://stephenrho.github.io/pminternal/news/index.html","id":"pminternal-010","dir":"Changelog","previous_headings":"","what":"pminternal 0.1.0","title":"pminternal 0.1.0","text":"CRAN release: 2025-03-18 Initial CRAN submission.","code":""}]
