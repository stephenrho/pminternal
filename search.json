[{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting started with `pminternal`","text":"developing clinical prediction model measures model performance biased fact using data fit (‘train’) model evaluate . Splitting data development validation sets inefficient. Bootstrapping cross-validation can used estimate bias-corrected measures model performance. known ‘internal validation’ addresses question: expected performance model developed way sample selected population? confused ‘external validation’ assesses model performance different population. pminternal inspired functions validate predab.resample rms package. aim provide package work user-defined model development procedure (assuming can implemented R function). package also implements recently proposed ‘stability plots’. Currently binary outcomes supported goal eventually extend outcomes (survival, ordinal).","code":""},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"supplying-a-model-via-fit","dir":"Articles","previous_headings":"","what":"Supplying a model via fit","title":"Getting started with `pminternal`","text":"validate needs single argument run, fit. fit fitted model compatible insight::get_data, insight::find_response, insight::get_call, marginaleffects::get_predict. Models supported insight can found running insight::supported_models() (run is_model_supported(fit)); models supported marginaleffects https://marginaleffects.com/articles/supported_models.html. ’re dealing binary outcomes, models listed applicable. code loads GUSTO-trial data, selects relevant variables, downsamples reduce run time, fits development model (glm), passes validate. validate call run method = \"boot_optimism\" able assess model stability via following calls. Note stability plots based estimates optimism rather based predictions models developed bootstrapped resampled data sets evaluated original/development data. sense conceptually related bias-corrected estimates obtained method = \"boot_simple\". case methods results necessary data make plots (see also classification_stability dcurve_stability).    final part example. possible get apparent bias-corrected calibration curves. need set additional argument, specifying assess calibration curve (.e., points x-axis) follows.  plotting functions fairly basic invisibly return data needed reproduce like. example, plot uses ggplot2 adds histogram predicted risk probabilities (stored p) show distribution.  Additional models supplied via fit tested gusto example given . Please let know run trouble model class feel work fit. chunk evaluated build time print output.","code":"library(pminternal) library(Hmisc) #>  #> Attaching package: 'Hmisc' #> The following objects are masked from 'package:base': #>  #>     format.pval, units  getHdata(\"gusto\") gusto <- gusto[, c(\"sex\", \"age\", \"hyp\", \"htn\", \"hrt\", \"pmi\", \"ste\", \"day30\")]  gusto$y <- gusto$day30; gusto$day30 <- NULL  set.seed(234) gusto <- gusto[sample(1:nrow(gusto), size = 5000),]  mod <- glm(y ~ ., data = gusto, family = \"binomial\")  mod_iv <- validate(mod, B = 20) #> It is recommended that B >= 200 for bootstrap validation mod_iv #>                 C    Brier Intercept  Slope    Eavg    E50    E90  Emax #> Apparent  0.79959  0.05992     0.000 1.0000 0.00385 0.0023 0.0076 0.093 #> Optimism  0.00099 -0.00046     0.014 0.0082 0.00092 0.0012 0.0032 0.035 #> Corrected 0.79860  0.06038    -0.014 0.9918 0.00293 0.0011 0.0044 0.057 #>                ECI #> Apparent   0.00521 #> Optimism   0.00578 #> Corrected -0.00057 # prediction stability plot with 95% 'stability interval' prediction_stability(mod_iv, bounds = .95) # calibration stability  # (using default calibration curve arguments: see pminternal:::cal_defaults()) calibration_stability(mod_iv) # mean absolute prediction error (mape) stability  # mape = average difference between boot model predictions # for original data and original model mape <- mape_stability(mod_iv) mape$average_mape #> [1] 0.007446025 # find 100 equally spaced points  # between the lowest and highest risk prediction p <- predict(mod, type=\"response\")  p_range <- seq(min(p), max(p), length.out=100)  mod_iv2 <- validate(mod, B = 20, calib_args=list(eval=p_range)) #> It is recommended that B >= 200 for bootstrap validation mod_iv2 #>                C    Brier Intercept Slope    Eavg     E50    E90  Emax     ECI #> Apparent  0.7996  0.05992     0.000 1.000 0.00385 0.00232 0.0076 0.093 0.00521 #> Optimism  0.0038 -0.00068     0.045 0.025 0.00047 0.00158 0.0027 0.030 0.00093 #> Corrected 0.7957  0.06060    -0.045 0.975 0.00338 0.00074 0.0049 0.062 0.00428  calp <- cal_plot(mod_iv2) head(calp) #>     predicted    apparent bias_corrected #> 1 0.001639574 0.001092496    0.000854699 #> 2 0.009714890 0.007787001    0.007759467 #> 3 0.017790205 0.015366875    0.019001038 #> 4 0.025865521 0.023634527    0.027067193 #> 5 0.033940837 0.032389166    0.034308507 #> 6 0.042016153 0.041466700    0.043179498  library(ggplot2)  ggplot(calp, aes(x=predicted)) +   geom_abline(lty=2) +   geom_line(aes(y=apparent, color=\"Apparent\")) +   geom_line(aes(y=bias_corrected, color=\"Bias-Corrected\")) +   geom_histogram(data = data.frame(p = p), aes(x=p, y=after_stat(density)*.01),                  binwidth = .001, inherit.aes = F, alpha=1/2) +   labs(x=\"Predicted Risk\", y=\"Estimated Risk\", color=NULL) ### generalized boosted model with gbm library(gbm) # syntax y ~ . does not work with gbm mod <- gbm(y ~ sex + age + hyp + htn + hrt + pmi + ste,             data = gusto, distribution = \"bernoulli\", interaction.depth = 2)  (gbm_iv <- validate(mod, B = 20))  ### generalized additive model with mgcv library(mgcv)  mod <- gam(y ~ sex + s(age) + hyp + htn + hrt + pmi + ste,             data = gusto, family = \"binomial\")  (gam_iv <- validate(mod, B = 20))  mod <- bam(y ~ sex + s(age, bs = \"cr\") + hyp + htn + hrt + pmi + ste,             data = gusto, family = \"binomial\")  (bam_iv <- validate(mod, B = 20))  ### rms implementation of logistic regression mod <- rms::lrm(y ~ ., data = gusto)  # not loading rms to avoid conflict with rms::validate...  (lrm_iv <- validate(mod, B = 20))"},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"user-defined-model-development-functions","dir":"Articles","previous_headings":"","what":"User-defined model development functions","title":"Getting started with `pminternal`","text":"important internally validated entire model development procedure, including tuning hyperparameters, variable selection, . Often fit object capture (supported). example work model supported insight marginaleffects: logistic regression lasso (L1) regularization. functions need specify model_fun pred_fun. model_fun take single argument, data, return object can used make predictions pred_fun. ... also added argument allow optional arguments passed validate (see vignette(“pminternal-examples”) examples user-defined functions take optional arguments). lasso_fun formats data glmnet, selects hyperparameter, lambda (controls degree regularization), via 10-fold cross-validation, fits final model ‘best’ value lambda returns. pred_fun take two arguments, model data, well optional argument(s) .... pred_fun work model object returned model_fun. glmnet objects predict method function lasso_predict simply formats data returns predictions. predict.glmnet returns matrix select first column return vector predicted risks. recommend use :: refer functions particular packages want run bootstrapping parallel. cores = 1 (cores argument supplied) cross-validation issue can use library. code tests functions gusto. work intended can pass functions validate follows. using cross-validation estimate optimism. Note 10-fold cross-validation select best value lambda (.e., hyperparameter tuning) done fold performed validate.  examples user defined model functions (including elastic net random forest) can found vignette(\"validate-examples\").","code":"#library(glmnet)  lasso_fun <- function(data, ...){   y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      cv <- glmnet::cv.glmnet(x=x, y=y, alpha=1, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet::glmnet(x=x, y=y, alpha = 1, lambda = lambda, family=\"binomial\") }  lasso_predict <- function(model, data, ...){   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      plogis(glmnet::predict.glmnet(model, newx = x)[,1]) } lasso_app <- lasso_fun(gusto) lasso_p <- lasso_predict(model = lasso_app, data = gusto) # for calibration plot eval <- seq(min(lasso_p), max(lasso_p), length.out=100)  iv_lasso <- validate(method = \"cv_optimism\", data = gusto,                       outcome = \"y\", model_fun = lasso_fun,                       pred_fun = lasso_predict, B = 10,                       calib_args=list(eval=eval))  iv_lasso #>                C    Brier Intercept Slope    Eavg     E50     E90   Emax #> Apparent  0.7995  0.05990     0.036 1.017  0.0039  0.0028  0.0079  0.082 #> Optimism  0.0055 -0.00052     0.065 0.024 -0.0132 -0.0066 -0.0320 -0.143 #> Corrected 0.7940  0.06042    -0.029 0.993  0.0171  0.0094  0.0400  0.225 #>               ECI #> Apparent   0.0041 #> Optimism  -0.1224 #> Corrected  0.1264  cal_plot(iv_lasso)"},{"path":"https://stephenrho.github.io/pminternal/articles/pminternal.html","id":"user-defined-score-functions","dir":"Articles","previous_headings":"","what":"User-defined score functions","title":"Getting started with `pminternal`","text":"scores returned score_binary enough clinical prediction model applications sometimes different measures may desired. can achieved specifying score_fun. take two arguments, y p, can take optional arguments. score_fun return named vector scores calculated y p. function sens_spec takes optional argument threshold used calculate sensitivity specificity. threshold specified set 0.5. call validate uses glm fit beginning vignette uses sens_spec function calculate bias-corrected sensitivity specificity threshold 0.2 (case assessing classification stability important).","code":"sens_spec <- function(y, p, ...){   # this function supports an optional   # arg: threshold (set to .5 if not specified)   dots <- list(...)   if (\"threshold\" %in% names(dots)){     thresh <- dots[[\"threshold\"]]   } else{     thresh <- .5   }   # make sure y is 1/0   if (is.logical(y)) y <- as.numeric(y)   # predicted 'class'   pcla <- as.numeric(p > thresh)     sens <- sum(y==1 & pcla==1)/sum(y==1)   spec <- sum(y==0 & pcla==0)/sum(y==0)    scores <- c(sens, spec)   names(scores) <- c(\"Sensitivity\", \"Specificity\")      return(scores) } validate(fit = mod, score_fun = sens_spec, threshold=.2,          method = \"cv_optimism\", B = 10) #>           Sensitivity Specificity #> Apparent       0.3045     0.93990 #> Optimism       0.0074     0.00012 #> Corrected      0.2971     0.93978"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"backward-selection","dir":"Articles","previous_headings":"","what":"Backward Selection","title":"More `model_fun` examples for `pminternal::validate`","text":"function implements model selected via backward elimination using AIC. situation probably best stick lrm, fastbw, validate rms package (though note differences default step behavior) unless want additional calibration metrics offered pminternal want specify score function (see vignette(\"pminternal\")).","code":"stepglm <- function(data, ...){   m <- glm(y~., data=data, family=\"binomial\")   step(m, trace = 0) }  steppred <- function(model, data, ...){   predict(model, newdata = data, type = \"response\") }  validate(data = gusto, outcome = \"y\", model_fun = stepglm,           pred_fun = steppred, method = \"cv_opt\", B = 10) #>                C    Brier Intercept Slope    Eavg     E50     E90   Emax #> Apparent  0.7996  0.05992     0.000 1.000  0.0039  0.0023  0.0076  0.093 #> Optimism  0.0066 -0.00059     0.077 0.029 -0.0133 -0.0066 -0.0334 -0.137 #> Corrected 0.7930  0.06051    -0.077 0.971  0.0171  0.0089  0.0410  0.229 #>               ECI #> Apparent   0.0052 #> Optimism  -0.1292 #> Corrected  0.1344"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"ridge","dir":"Articles","previous_headings":"","what":"Ridge","title":"More `model_fun` examples for `pminternal::validate`","text":"vignette(\"pminternal\") gives example glm lasso (L1) penalization. simple modify implement ridge (L2) penalization setting alpha = 0. Rather two separate functions specify optional argument, alpha, supplied validate. argument isn’t supplied function defaults alpha = 0. chunk evaluated output printed.","code":"#library(glmnet)  ridge_fun <- function(data, ...){   y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      cv <- glmnet::cv.glmnet(x=x, y=y, alpha=0, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet::glmnet(x=x, y=y, alpha = 0, lambda = lambda, family=\"binomial\") }  ridge_predict <- function(model, data, ...){   # note this is identical to lasso_predict from \"pminternal\" vignette   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      plogis(glmnet::predict.glmnet(model, newx = x)[,1]) }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = ridge_fun,           pred_fun = ridge_predict, B = 10) #>                C    Brier Intercept Slope   Eavg     E50    E90   Emax     ECI #> Apparent  0.7998  0.05991     0.204 1.095  0.006  0.0054  0.013  0.048  0.0052 #> Optimism  0.0054 -0.00047     0.066 0.025 -0.011 -0.0052 -0.024 -0.144 -0.1100 #> Corrected 0.7944  0.06037     0.138 1.071  0.016  0.0105  0.037  0.192  0.1152  # the use of package::function in user defined functions  # is especially important if you want to run  # boot_* or .632 in parallel via cores argument  # e.g. # validate(method = \".632\", data = gusto,  #          outcome = \"y\", model_fun = ridge_fun,  #          pred_fun = ridge_predict, B = 100, cores = 4) lognet_fun <- function(data, ...){      dots <- list(...)   if (\"alpha\" %in% names(dots)){     alpha <- dots[[\"alpha\"]]   } else{     alpha <- 0   }   # cat(\"Using alpha =\", alpha, \"\\n\")      y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      cv <- glmnet::cv.glmnet(x=x, y=y, alpha = alpha, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet::glmnet(x=x, y=y, alpha = alpha, lambda = lambda, family=\"binomial\") }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = lognet_fun,           pred_fun = ridge_predict, B = 10, alpha = 0.5)"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"elastic-net","dir":"Articles","previous_headings":"","what":"Elastic Net","title":"More `model_fun` examples for `pminternal::validate`","text":"implement model elastic net penalty need add steps select alpha. function evaluates nalpha equally spaced values alpha 0 1 (inclusive) selects values lambda alpha result minimum CV binomial deviance (changed via type.measure). nalpha optional argument. Note don’t need new predict function ridge_predict used. save build time chunk evaluated.","code":"enet_fun <- function(data, ...){      dots <- list(...)   if (\"nalpha\" %in% names(dots)){     nalpha <- dots[[\"nalpha\"]]   } else{     nalpha <- 21 # 0 to 1 in steps of 0.05   }      y <- data$y   x <- data[, c('sex', 'age', 'hyp', 'htn', 'hrt', 'pmi', 'ste')]   x$sex <- as.numeric(x$sex == \"male\")   x$pmi <- as.numeric(x$pmi == \"yes\")   x <- as.matrix(x)      # run 10 fold CV for each alpha   alphas <- seq(0, 1, length.out = nalpha)   res <- lapply(alphas, function(a){     cv <- glmnet::cv.glmnet(x=x, y=y, alpha = a, nfolds = 10, family=\"binomial\")     list(lambda = cv$lambda.min, bin.dev = min(cv$cvm))   })   # select result with min binomial deviance   j <- which.min(sapply(res, function(x) x$bin.dev))   # produce 'final' model with alpha and lambda   glmnet::glmnet(x=x, y=y, alpha = alphas[j], lambda = res[[j]][[\"lambda\"]], family=\"binomial\") }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = enet_fun,           pred_fun = ridge_predict, B = 10)"},{"path":"https://stephenrho.github.io/pminternal/articles/validate-examples.html","id":"random-forest","dir":"Articles","previous_headings":"","what":"Random Forest","title":"More `model_fun` examples for `pminternal::validate`","text":"example use ranger package create model_fun allow optional arguments num.trees, max.depth, min.node.size; others added (see ?ranger).","code":"# library(ranger)  rf_fun <- function(data, ...){      dots <- list(...)   num.trees <- if (\"num.trees\" %in% names(dots)) dots[[\"num.trees\"]] else 500   max.depth <- if (\"max.depth\" %in% names(dots)) dots[[\"max.depth\"]] else NULL   min.node.size <- if (\"min.node.size\" %in% names(dots)) dots[[\"min.node.size\"]] else 1      # best to make sure y is a factor where '1' is level 2   data$y <- factor(data$y, levels = 0:1)      ranger::ranger(y~., data = data, probability = T,                   num.trees = num.trees,                   max.depth = max.depth,                   min.node.size = min.node.size) }  rf_predict <- function(model, data, ...){   predict(model, data = data)$predictions[, 2]  }  validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = rf_fun,           pred_fun = rf_predict, B = 10) #>              C  Brier Intercept Slope  Eavg   E50   E90 Emax   ECI #> Apparent  0.96  0.045      3.60  3.06 0.055 0.031 0.067 0.56 1.094 #> Optimism  0.18 -0.017      3.82  2.19 0.042 0.024 0.036 0.40 1.082 #> Corrected 0.78  0.062     -0.22  0.88 0.014 0.007 0.031 0.16 0.012  # instead of unlimited tree depth... validate(method = \"cv_optimism\", data = gusto,           outcome = \"y\", model_fun = rf_fun,           pred_fun = rf_predict, B = 10, max.depth = 3) #>               C   Brier Intercept Slope    Eavg     E50     E90 Emax    ECI #> Apparent  0.817  0.0603      2.22  1.99 0.03055 0.02283  0.0533 0.38  0.216 #> Optimism  0.022 -0.0012      0.46  0.21 0.00069 0.00064 -0.0013 0.12 -0.011 #> Corrected 0.794  0.0615      1.76  1.78 0.02987 0.02218  0.0545 0.26  0.227"},{"path":"https://stephenrho.github.io/pminternal/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Stephen Rhodes. Author, maintainer, copyright holder.","code":""},{"path":"https://stephenrho.github.io/pminternal/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Rhodes S (2023). pminternal: Internal Validation Clinical Prediction Models. R package version 0.0.1, https://stephenrho.github.io/pminternal/, https://github.com/stephenrho/pminternal.","code":"@Manual{,   title = {pminternal: Internal Validation of Clinical Prediction Models},   author = {Stephen Rhodes},   year = {2023},   note = {R package version 0.0.1, https://stephenrho.github.io/pminternal/},   url = {https://github.com/stephenrho/pminternal}, }"},{"path":"https://stephenrho.github.io/pminternal/index.html","id":"pminternal-internal-validation-of-clinical-prediction-models","dir":"","previous_headings":"","what":"Internal Validation of Clinical Prediction Models","title":"Internal Validation of Clinical Prediction Models","text":"goal offer package can produce bias-corrected performance measures binary outcomes range model development approaches available R (similar rms::validate). Also contains functions assessing prediction stability described https://doi.org/10.1002/bimj.202200302. install development version: Please send feedback steverho89@gmail.com open issue.","code":"# install.packages(\"devtools\") devtools::install_github(\"stephenrho/pminternal\", build_vignettes = TRUE)"},{"path":"https://stephenrho.github.io/pminternal/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Internal Validation of Clinical Prediction Models","text":"example use bootstrapping correct performance measures glm via calculation ‘optimism’ (see vignette(\"pminternal\") vignette(\"validate-examples\") examples): available methods calculating bias corrected performance simple bootstrap (boot_simple), 0.632 bootstrap optimism (.632), optimism via cross-validation (cv_optimism), regular cross-validation (cv_average). Please see ?pminternal::validate references therein. Bias corrected calibration curves can also produced (see cal_plot). models supported via fit, users able specify model (model_fun) prediction (pred_fun) functions shown . Note specifying user-defined model prediction functions data outcome must also provided. crucial model_fun implements entire model development procedure (variable selection, hyperparameter tuning, etc). examples, see vignette(\"pminternal\") vignette(\"validate-examples\"). output validate (method = \"boot_*\") can used produce plots assessing stability model predictions (across models developed bootstrap resamples). prediction ()stability plot shows predictions B (case 100) bootstrap models applied development data.  MAPE plot shows mean absolute prediction error, difference predicted risk development model B bootstrap models.  calibration ()stability plot depict original calibration curve along B calibration curves bootstrap models applied original data (y).  classification instability index (CII) proportion individuals change predicted class (present/absent, 1/0) predicted risk compared threshold. example, patient predicted class 1 receive CII 0.3 30% bootstrap models led predicted class 0.  Decision curves implied original bootstrap models can also plotted.","code":"library(pminternal)  # make some data set.seed(2345) n <- 800 p <- 10  X <- matrix(rnorm(n*p), nrow = n, ncol = p) LP <- -1 + apply(X[, 1:5], 1, sum) # first 5 variables predict outcome y <- rbinom(n, 1, plogis(LP))  dat <- data.frame(y, X)  # fit a model mod <- glm(y ~ ., data = dat, family = \"binomial\")  # calculate bootstrap optimism corrected performance measures (val <- validate(fit = mod, method = \"boot_optimism\", B = 100)) #> It is recommended that B >= 200 for bootstrap validation #>                C   Brier Intercept Slope    Eavg     E50     E90    Emax #> Apparent  0.8567  0.1423     0.000 1.000  0.0045  0.0039  0.0081  0.0109 #> Optimism  0.0093 -0.0054     0.017 0.053 -0.0048 -0.0050 -0.0107 -0.0057 #> Corrected 0.8474  0.1477    -0.017 0.947  0.0093  0.0089  0.0187  0.0165 #>               ECI #> Apparent   0.0027 #> Optimism  -0.0038 #> Corrected  0.0065 # fit a glm with lasso penalty library(glmnet) #> Loading required package: Matrix #> Loaded glmnet 4.1-7  lasso_fun <- function(data, ...){   y <- data$y   x <- as.matrix(data[, which(colnames(data) != \"y\")])      cv <- cv.glmnet(x=x, y=y, alpha=1, nfolds = 10, family=\"binomial\")   lambda <- cv$lambda.min      glmnet(x=x, y=y, alpha = 1, lambda = lambda, family=\"binomial\") }  lasso_predict <- function(model, data, ...){   y <- data$y   x <- as.matrix(data[, which(colnames(data) != \"y\")])      predict(model, newx = x, type = \"response\")[,1] }  (val <- validate(data = dat, outcome = \"y\",                   model_fun = lasso_fun, pred_fun = lasso_predict,                   method = \"boot_optimism\", B = 100)) #> It is recommended that B >= 200 for bootstrap validation #>                C   Brier Intercept Slope   Eavg    E50    E90  Emax   ECI #> Apparent  0.8558  0.1427     0.073  1.14 0.0184 0.0178 0.0366 0.040 0.044 #> Optimism  0.0062 -0.0037     0.015  0.04 0.0025 0.0033 0.0039 0.014 0.016 #> Corrected 0.8496  0.1463     0.057  1.10 0.0159 0.0144 0.0326 0.026 0.028 prediction_stability(val, smooth_bounds = TRUE) mape_stability(val) calibration_stability(val) classification_stability(val, threshold = .4) dcurve_stability(val)"},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"Estimate bias-corrected scores via calculation bootstrap optimism (standard .632). Can also produce estimates assessing stability prediction model predictions. function called validate.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"","code":"boot_optimism(   data,   outcome,   model_fun,   pred_fun,   score_fun,   method = c(\"boot\", \".632\"),   B = 200,   ... )"},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"data data used developing model. contain variables considered (.e., even excluded variable selection development sample) outcome character denoting column name outcome data. model_fun function takes least one argument, data. function implement entire model development procedure (.e., hyperparameter tuning, variable selection, imputation). Additional arguments can provided via .... function return object works pred_fun. pred_fun function takes least two arguments, model data. function return numeric vector predicted probabilities outcome length number rows data important take account missing data treated (e.g., predict.glm omits predictions rows missing values). score_fun function calculate metrics interest. specified score_binary used. method \"boot\" \".632\". former estimates bootstrap optimism score subtracts apparent scores (simple bootstrap estimates also produced product). latter estimates \".632\" optimism described Harrell (2015). See validate details. B number bootstrap resamples run (least 200) ... additional arguments model_fun, pred_fun, /score_fun.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"list class internal_boot containing: apparent - scores calculated original data using original model. optimism - estimates optimism score (average difference score bootstrap models evaluated bootstrap vs original sample) can subtracted 'apparent' performance calculated using original model original data. corrected - 'bias corrected' scores (apparent - optimism) simple - method = \"boot\", estimates scores derived 'simple bootstrap'. average score calculated bootstrap models evaluated original outcome data. NULL method = \".632\" stability - method = \"boot\", N,B matrix N number observations data B number bootstrap samples. column contains predicted probabilities outcome bootstrap model evaluated original data. NULL method = \".632\"","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation predictive models: efficiency procedures logistic regression analysis. Journal clinical epidemiology, 54(8), 774-781. Harrell Jr F. E. (2015). Regression Modeling Strategies: applications linear models, logistic ordinal regression, survival analysis. New York: Springer Science, LLC.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/boot_optimism.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate optimism and bias-corrected scores via bootstrap resampling — boot_optimism","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 1000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.186 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model #m1 <- glm(y ~ x1 + x2, data=dat, family=\"binomial\")  model_fun <- function(data, ...){   glm(y ~ x1 + x2, data=data, family=\"binomial\") }  pred_fun <- function(model, data, ...){   predict(model, newdata=data, type=\"response\") }  boot_optimism(data=dat, outcome=\"y\", model_fun=model_fun, pred_fun=pred_fun,               method=\"boot\", B=20) # B set to 20 for example but should be >= 200 #>                C   Brier Intercept Slope   Eavg    E50    E90  Emax   ECI #> Apparent  0.7964  0.1262   6.6e-15 1.000 0.0195 0.0162 0.0328 0.101 0.062 #> Optimism  0.0049 -0.0018   1.0e-02 0.016 0.0044 0.0051 0.0076 0.020 0.048 #> Corrected 0.7915  0.1280  -1.0e-02 0.984 0.0151 0.0111 0.0251 0.081 0.014"},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default settings for calibration curves — cal_defaults","title":"Get default settings for calibration curves — cal_defaults","text":"Get default settings calibration curves","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default settings for calibration curves — cal_defaults","text":"","code":"cal_defaults(x = NULL)"},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default settings for calibration curves — cal_defaults","text":"x ignored","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_defaults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default settings for calibration curves — cal_defaults","text":"list containing default arguments supply pmcalibration::pmcalibration","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot apparent and bias-corrected calibration curves — cal_plot","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"Plot apparent bias-corrected calibration curves","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"","code":"cal_plot(x, xlim, ylim, xlab, ylab, app_col, bc_col, app_lty, bc_lty)"},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"x object returned validate. Original call specified 'eval' argument. See score_binary. xlim x limits (default = c(0, max either curve)) ylim y limits (default = c(0, max either curve)) xlab title x axis ylab title y axis app_col color apparent calibration curve (default = 'black') bc_col color bias-corrected calibration curve (default = 'black') app_lty line type apparent calibration curve (default = 1) bc_lty line type bias-corrected calibration curve (default = 2)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"plots apparent bias-corrected curves. Silently returns data.frame can used produce 'publication ready' plot. Columns follows: predicted = values x-axis, apparent = value apparent curve, bias_corrected = value bias-corrected curve.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/cal_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot apparent and bias-corrected calibration curves — cal_plot","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ x1 + x2, data=dat, family=\"binomial\")  # to get a plot of bias-corrected calibration we need # to specify 'eval' argument via 'calib_args' # this argument specifies at what points to evalulate the # calibration curve for plotting. The example below uses # 100 equally spaced points between the min and max # original prediction.  p <- predict(m1, type=\"response\") p100 <- seq(min(p), max(p), length.out=100)  m1_iv <- validate(m1, method=\"cv_optimism\", B=10,                   calib_args = list(eval=p100)) # calib_ags can be used to set other calibration curve # settings: see pmcalibration::pmcalibration  cal_plot(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot calibration stability across bootstrap replicates — calibration_stability","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"calibration ()stability plot shows calibration curves bootstrap models evaluated original outcome. stable model produce boot calibration curves differ minimally 'apparent' curve. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"","code":"calibration_stability(x, calib_args, xlim, ylim, xlab, ylab, col)"},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"x object produced validate method = \"boot_\\*\" (boot_optimism method=\"boot\") calib_args settings calibration curve (see pmcalibration::pmcalibration). unspecified settings given cal_defaults 'eval' set 100 (evaluate curve 100 points min max prediction). xlim x limits (default = c(0,1)) ylim y limits (default = c(0,1)) xlab title x axis ylab title y axis col color lines bootstrap models (default = grDevices::grey(.5, .3))","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"plots calibration ()stability. Invisibly returns list containing data curve (p=x-axis, pc=y-axis). first element list apparent curve (original model original outcome).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"Riley RD, Collins GS. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biom J. doi:10.1002/bimj.202200302. Epub ahead print.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/calibration_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot calibration stability across bootstrap replicates — calibration_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  calibration_stability(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification instability plot — classification_stability","title":"Classification instability plot — classification_stability","text":"Classification instability plot shows relationship original model estimated risk classification instability index (CII). CII proportion bootstrap replicates predicted class (0 p <= threshold; 1 p > threshold) different obtained original model. risk predictions around threshold exhibit elevated CII unstable model exhibit high CII across range risk predictions. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification instability plot — classification_stability","text":"","code":"classification_stability(x, threshold, xlim, ylim, xlab, ylab, pch, cex, col)"},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification instability plot — classification_stability","text":"x object produced validate method = \"boot_\\*\" (boot_optimism method=\"boot\") threshold estimated risks threshold get predicted 'class' 1, otherwise 0. xlim x limits (default = range estimated risks) ylim y limits (default = c(0, maximum CII)) xlab title x axis ylab title y axis pch plotting character (default = 16) cex controls point size (default = 1) col color points (default = grDevices::grey(.5, .5))","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classification instability plot — classification_stability","text":"plots classification ()stability. Invisibly returns estimates CII observation.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Classification instability plot — classification_stability","text":"Riley RD, Collins GS. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biom J. doi:10.1002/bimj.202200302. Epub ahead print.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/classification_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classification instability plot — classification_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  classification_stability(m1_iv, threshold=.2)"},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate bias-corrected scores via cross-validation — crossval","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"Estimate bias-corrected scores via cross-validation. CV used calculate optimism subtracted apparent scores calculate average performance sample (held ) data. function called validate.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"","code":"crossval(data, outcome, model_fun, pred_fun, score_fun, k = 10, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"data data used developing model. contain variables considered (.e., even excluded variable selection development sample) outcome character denoting column name outcome data. model_fun function takes least one argument, data. function implement entire model development procedure (.e., hyperparameter tuning, variable selection, imputation). Additional arguments can provided via .... function return object works pred_fun. pred_fun function takes least two arguments, model data. function return numeric vector predicted probabilities outcome length number rows data important take account missing data treated (e.g., predict.glm omits predictions rows missing values). score_fun function calculate metrics interest. specified score_binary used. k number folds. Typically scores need >> 2 observations calculated folds chosen mind. ... additional arguments model_fun, pred_fun, /score_fun.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"list class internal_cv containing: apparent - scores calculated original data using original model. optimism - estimates optimism score (average difference score training data vs test data fold) can subtracted 'apparent' performance calculated using original model original data. cv_optimism_corrected - 'bias corrected' scores (apparent - optimism). produced rms::validate, rms::predab.resample. cv_average - average scores calculated test (held ) data. metric described Steyerberg et al. (2001). indices - indices used define test set fold.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation predictive models: efficiency procedures logistic regression analysis. Journal clinical epidemiology, 54(8), 774-781.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/crossval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate bias-corrected scores via cross-validation — crossval","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 1000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.186 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model #m1 <- glm(y ~ x1 + x2, data=dat, family=\"binomial\")  model_fun <- function(data, ...){   glm(y ~ x1 + x2, data=data, family=\"binomial\") }  pred_fun <- function(model, data, ...){   predict(model, newdata=data, type=\"response\") }  # CV Corrected = Apparent - CV Optimism # CV Average = average score in held out fold crossval(data=dat, outcome=\"y\", model_fun=model_fun, pred_fun=pred_fun, k=10) #>                    C   Brier Intercept  Slope   Eavg    E50    E90  Emax    ECI #> Apparent      0.7964  0.1262   6.6e-15  1.000  0.020  0.016  0.033  0.10  0.062 #> CV Optimism  -0.0024 -0.0013   2.7e-02 -0.033 -0.037 -0.029 -0.077 -0.11 -0.561 #> CV Corrected  0.7988  0.1275  -2.7e-02  1.033  0.056  0.045  0.110  0.21  0.622 #> CV Average    0.7989  0.1274  -2.7e-02  1.033  0.056  0.045  0.110  0.21  0.624"},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot decision curve stability across bootstrap replicates — dcurve_stability","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"decision curve ()stability plot shows decision curves bootstrap models evaluated original outcome. stable model produce curves differ minimally 'apparent' curve. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"","code":"dcurve_stability(   x,   thresholds = seq(0, 0.99, by = 0.01),   xlim,   ylim,   xlab,   ylab,   col )"},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"x object produced validate method = \"boot_\\*\" (boot_optimism method=\"boot\") thresholds points evaluate decision curves (see dcurves::dca) xlim x limits (default = range thresholds) ylim y limits (default = range net benefit) xlab title x axis ylab title y axis col color points (default = grDevices::grey(.5, .5))","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"plots decision curve ()stability. Invisibly returns list containing data curve. returned dcurves::dca. first element list apparent curve (original model original outcome).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"Riley RD, Collins GS. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biom J. doi:10.1002/bimj.202200302. Epub ahead print.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/dcurve_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot decision curve stability across bootstrap replicates — dcurve_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  dcurve_stability(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Get stability from internal_validate or internal_boot object — get_stability","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"Get stability internal_validate internal_boot object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"","code":"get_stability(x)"},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"x internal_validate (see validate) internal_boot (see boot_optimism) object. former created method = \"boot_optimism\" \"boot_simple\".","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/get_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get stability from internal_validate or internal_boot object — get_stability","text":"list containing stability matrix (n x B+1 matrix. column contains predictions p observations. First column contains predictions original model. columns bootstrap models) original binary outcome (y).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean absolute predictor error (MAPE) stability plot — mape_stability","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"MAPE ()stability plot shows mean absolute predictor error (average absolute difference original estimated risk risk B bootstrap models) function apparent estimated risk (prediction original/development model). See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"","code":"mape_stability(x, xlim, ylim, xlab, ylab, pch, cex, col)"},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"x object produced validate method = \"boot_\\*\" (boot_optimism method=\"boot\") xlim x limits (default = range estimated risks) ylim y limits (default = c(0, maximum mape)) xlab title x axis ylab title y axis pch plotting character (default = 16) cex controls point size (default = 1) col color points (default = grDevices::grey(.5, .5))","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"plots calibration ()stability. Invisibly returns list containing data curve (p=x-axis, pc=y-axis). first element list apparent curve (original model original outcome).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"Riley RD, Collins GS. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biom J. doi:10.1002/bimj.202200302. Epub ahead print.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/mape_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mean absolute predictor error (MAPE) stability plot — mape_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  mape_stability(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/pminternal-package.html","id":null,"dir":"Reference","previous_headings":"","what":"pminternal: Internal Validation of Clinical Prediction Models — pminternal-package","title":"pminternal: Internal Validation of Clinical Prediction Models — pminternal-package","text":"Conduct internal validation clinical prediction model binary outcome. Produce bias corrected performance metrics (c-statistic, calibration slope) via bootstrap (simple bootstrap, bootstrap optimism, .632 optimism) crossvalidation (CV optimism, CV average). Also includes functions assess model stability via bootstrap resampling. See Steyerberg et al. (2001) doi:10.1016/s0895-4356(01)00341-9 ; Harrell (2015) doi:10.1007/978-3-319-19425-7 ; Riley Collins (2023) doi:10.1002/bimj.202200302","code":""},{"path":[]},{"path":"https://stephenrho.github.io/pminternal/reference/pminternal-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pminternal: Internal Validation of Clinical Prediction Models — pminternal-package","text":"Maintainer: Stephen Rhodes steverho89@gmail.com [copyright holder]","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot prediction stability across bootstrap replicates — prediction_stability","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"prediction ()stability plot shows estimated risk probabilities models developed resampled data evaluated original development data function 'apparent' prediction (prediction original/development model evaluated original data). stable model produce points exhibit minimal dispersion. See Riley Collins (2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"","code":"prediction_stability(   x,   bounds = 0.95,   smooth_bounds = FALSE,   xlab,   ylab,   pch,   cex,   col,   lty,   span )"},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"x object produced validate method = \"boot_\\*\" (boot_optimism method=\"boot\") bounds width 'stability interval' (percentiles bootstrap model predictions). NULL = add bounds plot. smooth_bounds TRUE, use loess smooth bounds (default = FALSE) xlab title x axis ylab title y axis pch plotting character (default = 16) cex controls point size (default = 0.4) col color points (default = grDevices::grey(.5, .5)) lty line type bounds (default = 2) span controls degree smoothing (see loess; default = 0.75)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"plots prediction ()stability. stability bounds smoothed. Invisibly returns stability matrix (column 1 original predictions) can used creating plots packages/software.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"Riley RD, Collins GS. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biom J. doi:10.1002/bimj.202200302. Epub ahead print.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/prediction_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot prediction stability across bootstrap replicates — prediction_stability","text":"","code":"set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation  prediction_stability(m1_iv)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a internal_boot object — print.internal_boot","title":"Print a internal_boot object — print.internal_boot","text":"Print internal_boot object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a internal_boot object — print.internal_boot","text":"","code":"# S3 method for internal_boot print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a internal_boot object — print.internal_boot","text":"x object created boot_optimism digits number digits print (default = 2) ... additional arguments print","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_boot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a internal_boot object — print.internal_boot","text":"invisibly returns x prints estimates console","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a internal_cv object — print.internal_cv","title":"Print a internal_cv object — print.internal_cv","text":"Print internal_cv object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a internal_cv object — print.internal_cv","text":"","code":"# S3 method for internal_cv print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a internal_cv object — print.internal_cv","text":"x object created crossval digits number digits print (default = 2) ... additional arguments print","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a internal_cv object — print.internal_cv","text":"invisibly returns x prints estimates console","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"print a internal_validate object — print.internal_validate","title":"print a internal_validate object — print.internal_validate","text":"print internal_validate object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"print a internal_validate object — print.internal_validate","text":"","code":"# S3 method for internal_validate print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"print a internal_validate object — print.internal_validate","text":"x internal_validate object digits number digits print ... optional arguments passed print","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"print a internal_validate object — print.internal_validate","text":"prints summary","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Print summary of internal_validate object — print.internal_validatesummary","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"Print summary internal_validate object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"","code":"# S3 method for internal_validatesummary print(x, digits = 2, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"x internal_validatesummary object digits number digits print ... ignored","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/print.internal_validatesummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print summary of internal_validate object — print.internal_validatesummary","text":"invisible(x) - prints summary","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Score predictions for binary events — score_binary","title":"Score predictions for binary events — score_binary","text":"Calculate scores summarizing discrimination/calibration predictions observed binary events. score_fun defined calling validate function used.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Score predictions for binary events — score_binary","text":"","code":"score_binary(y, p, ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Score predictions for binary events — score_binary","text":"y vector containing binary outcome p vector predictions ... additional arguments. function supports calib_args optional argument. calib_args contain arguments pmcalibration::pmcalibration. calibration plot (apparent vs bias corrected calibration curves via cal_plot) desired argument 'eval' provided. points evaluate calibration curve boot resample crossvalidation fold. good option calib_args = list(eval = seq(min(p), max(p), length.=100)); p predictions original model evaluated original data. Dots can used supply additional arguments user-defined functions.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Score predictions for binary events — score_binary","text":"named vector scores (see Details)","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Score predictions for binary events — score_binary","text":"following measures returned named vector. C c-statistic (aka area ROC curve). Probability randomly selected observation y = 1 higher p compared randomly selected y = 0. Brier mean squared error - mean((y - p)^2) Intercept Intercept logistic calibration model: glm(y ~ 1 + qlogis(p), family=\"binomial\") Slope Slope logistic calibration model: glm(y ~ 1 + qlogis(p), family=\"binomial\") Eavg average absolute difference p calibration curve (aka integrated calibration index ICI). E50 median absolute difference p calibration curve E90 90th percentile absolute difference p calibration curve Emax maximum absolute difference p calibration curve ECI average squared difference p calibration curve. Estimated calibration index (Van Hoorde et al. 2015) cal_plot eval specified (via calib_args), values plotting apparent bias-corrected calibration curves returned (see cal_plot). default omitted summary printed (see summary.internal_validate). Logistic calibration calibration metrics non-linear calibration curves assessing 'moderate-calibration' (Eavg, E50, E90, Emax, ECI; see references) calculated via pmcalibration package. default settings can modified passing calib_args validate call. calib_args named list corresponding arguments pmcalibration::pmcalibration.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Score predictions for binary events — score_binary","text":"Austin PC, Steyerberg EW. (2019) Integrated Calibration Index (ICI) related metrics quantifying calibration logistic regression models. Statistics Medicine. 38, pp. 1–15. https://doi.org/10.1002/sim.8281 Van Hoorde, K., Van Huffel, S., Timmerman, D., Bourne, T., Van Calster, B. (2015). spline-based tool assess visualize calibration multiclass risk predictions. Journal Biomedical Informatics, 54, pp. 283-93 Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina M., Steyerberg E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/score_binary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Score predictions for binary events — score_binary","text":"","code":"p <- runif(100) y <- rbinom(length(p), 1, p) score_binary(y = y, p = p) #>          C      Brier  Intercept      Slope       Eavg        E50        E90  #> 0.83313325 0.16627180 0.10230809 1.09364881 0.01892466 0.02033055 0.03572761  #>       Emax        ECI  #> 0.03680029 0.05172805"},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a internal_validate object — summary.internal_validate","title":"Summarize a internal_validate object — summary.internal_validate","text":"Summarize internal_validate object","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a internal_validate object — summary.internal_validate","text":"","code":"# S3 method for internal_validate summary(object, ignore_scores = \"^cal_plot\", ...)"},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a internal_validate object — summary.internal_validate","text":"object created call validate ignore_scores string used identify scores omit summary. score_binary produces scores prefix 'cal_plot' calibration plot desired (see cal_plot) ignored default. ... ignored","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a internal_validate object — summary.internal_validate","text":"data.frame 3 rows (apparent score, optimism, bias-corrected score) one column per score. methods produce optimism estimate row may NA.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/summary.internal_validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize a internal_validate object — summary.internal_validate","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation summary(m1_iv) #>                C   Brier Intercept   Slope   Eavg    E50    E90   Emax    ECI #> Apparent  0.7779  0.1335     0.000 1.00000 0.0076 0.0064 0.0115  0.058  0.011 #> Optimism  0.0016 -0.0011    -0.019 0.00083 0.0052 0.0038 0.0088  0.078  0.037 #> Corrected 0.7764  0.1346     0.019 0.99917 0.0024 0.0026 0.0027 -0.020 -0.026"},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":null,"dir":"Reference","previous_headings":"","what":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"Performs internal validation prediction model development procedure via bootstrapping cross-validation. Many model types supported via insight marginaleffects packages users can supply user-defined functions implement model development procedure retrieve predictions. Bias-corrected scores estimates optimism (applicable) provided.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"","code":"validate(   fit,   method = c(\"boot_optimism\", \"boot_simple\", \".632\", \"cv_optimism\", \"cv_average\"),   data,   outcome,   model_fun,   pred_fun,   score_fun,   B,   ... )"},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"fit model object. fit given insight package used extract data, outcome, original model call. Therefore, important fit supported insight implements entire model development process (see Harrell 2015). fit given selection variables method give accurate bias-correction. Model predictions obtained via marginaleffects::get_predict type = \"response\" fit compatible function. fit provided arguments data, outcome, model_fun, pred_fun ignored. method bias-correction method. Valid options \"boot_optimism\", \"boot_simple\", \".632\", \"cv_optimism\", \"cv_average\". See details. data data.frame containing data used fit development model outcome character denoting column name outcome data model_fun models supplied via fit function takes one named argument: 'data' (function include ... among arguments). function implement entire model development procedure (hyperparameter tuning, variable selection, imputation etc) return object can used pred_fun. Additional arguments can supplied ... pred_fun models supplied via fit function takes two named arguments: 'model' 'data' (function include ... among arguments). 'model' object returned model_fun. function return vector predicted risk probabilities length number rows data. Additional arguments can supplied ... score_fun function used produce performance measures predicted risks observed binary outcome. take two named arguments: 'y' 'p' (function include ... among arguments). function return named vector scores. unspecified score_binary used good purposes. B number bootstrap replicates crossvalidation folds. unspecified B set 200 method = \"boot_\\*\"/\".632\", set 10 method = \"cv_\\*\". ... additional arguments user-defined functions. Arguments producing calibration curves can set via 'calib_args' named list (see cal_plot score_binary). method = \"boot_optimism\", \"boot_simple\", \".632\" users can specify cores argument (e.g., cores = 4) run bootstrap samples parallel.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"object class internal_validate containing apparent bias-corrected estimates performance scores. method = \"boot_*\" also contains results pertaining stability predictions across bootstrapped models (see Riley Collins, 2023).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"Internal validation can provide bias-corrected estimates performance (e.g., C-statistic/AUC) model development procedure (.e., expected performance procedure applied another sample size population; see references). several approaches producing bias-corrected estimates (see ). important fit model_fun provided implement entire model development procedure, including hyperparameter tuning /variable selection. Note validate little check missing values. fit supplied insight::get_data extract data used fit model usually result complete cases used. User-defined model predict functions can specified handle missing values among predictor variables. Currently user supplied data rows missing outcome values removed. method boot_optimism (default) estimates optimism score subtracts apparent score (score calculated original/development model evaluated original sample). new model fit using procedure using bootstrap resample. Scores calculated applying boot model boot sample (\\(S_{boot}\\)) original sample (\\(S_{orig}\\)) difference gives estimate optimism given resample (\\(S_{boot} - S_{orig}\\)). average optimism across B resamples subtracted apparent score produce bias corrected score. boot_simple implements simple bootstrap. B bootstrap models fit evaluated original data. average score across B replicates bias-corrected score. .632 implements Harrell's adaption Efron's .632 estimator binary outcomes (see rms::predab.resample rms::validate). case estimate optimism \\(0.632 \\times (S_{app} - mean(S_{omit} \\times w))\\) \\(S_{app}\\) apparent performance score \\(S_{omit}\\) score estimated using bootstrap model evaluated --sample observations \\(w\\) weights proportion observations omitted (see Harrell 2015, p. 115). cv_optimism estimate optimism via B-fold crossvalidation. Optimism average difference performance measure predictions made training vs test (held fold) data. approach implemented rms::validate method=\"crossvalidation\". cv_average bias corrected scores average scores calculated assessing model developed fold evaluated test/held data. approach described compared \"boot_optimism\" \".632\" Steyerberg et al. (2001).","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"Steyerberg, E. W., Harrell Jr, F. E., Borsboom, G. J., Eijkemans, M. J. C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation predictive models: efficiency procedures logistic regression analysis. Journal clinical epidemiology, 54(8), 774-781. Harrell Jr F. E. (2015). Regression Modeling Strategies: applications linear models, logistic ordinal regression, survival analysis. New York: Springer Science, LLC. Efron (1983). “Estimating error rate prediction rule: improvement cross-validation”. Journal American Statistical Association, 78(382):316-331 Van Calster, B., Steyerberg, E. W., Wynants, L., van Smeden, M. (2023). thing validated prediction model. BMC medicine, 21(1), 70. Riley RD, Collins GS. (2023). Stability clinical prediction models developed using statistical machine learning methods. Biom J. doi:10.1002/bimj.202200302. Epub ahead print.","code":""},{"path":"https://stephenrho.github.io/pminternal/reference/validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get bias-corrected performance measures via bootstrapping or cross-validation — validate","text":"","code":"library(pminternal) set.seed(456) # simulate data with two predictors that interact dat <- pmcalibration::sim_dat(N = 2000, a1 = -2, a3 = -.3) mean(dat$y) #> [1] 0.1985 dat$LP <- NULL # remove linear predictor  # fit a (misspecified) logistic regression model m1 <- glm(y ~ ., data=dat, family=\"binomial\")  # internal validation of m1 via bootstrap optimism with 10 resamples # B = 10 for example but should be >= 200 in practice m1_iv <- validate(m1, method=\"boot_optimism\", B=10) #> It is recommended that B >= 200 for bootstrap validation m1_iv #>                C   Brier Intercept   Slope   Eavg    E50    E90   Emax    ECI #> Apparent  0.7779  0.1335     0.000 1.00000 0.0076 0.0064 0.0115  0.058  0.011 #> Optimism  0.0016 -0.0011    -0.019 0.00083 0.0052 0.0038 0.0088  0.078  0.037 #> Corrected 0.7764  0.1346     0.019 0.99917 0.0024 0.0026 0.0027 -0.020 -0.026"}]
